{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第 10 章: ストリーミング処理とスキーマ進化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, LongType\n",
    "CATALOG = \"my_catalog\"\n",
    "CATALOG_URL = \"http://server:8181/\"\n",
    "S3_ENDPOINT = \"http://minio:9000\"\n",
    "SPARK_VERSION = pyspark.__version__\n",
    "SPARK_MINOR_VERSION = '.'.join(SPARK_VERSION.split('.')[:2])\n",
    "ICEBERG_VERSION = \"1.8.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .config(\"spark.jars.packages\", \n",
    "                f\"org.apache.iceberg:iceberg-spark-runtime-{SPARK_MINOR_VERSION}_2.12:{ICEBERG_VERSION},org.apache.iceberg:iceberg-aws-bundle:{ICEBERG_VERSION},org.apache.spark:spark-sql-kafka-0-10_2.12:{SPARK_VERSION}\")\n",
    "        .config(f\"spark.sql.catalog.{CATALOG}\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "        .config(f\"spark.sql.catalog.{CATALOG}.type\", \"rest\")\n",
    "        .config(f\"spark.sql.catalog.{CATALOG}.uri\", CATALOG_URL)\n",
    "        .config(f\"spark.sql.catalog.{CATALOG}.s3.endpoint\", S3_ENDPOINT)\n",
    "        .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "        .config(\"spark.sql.defaultCatalog\", \"my_catalog\")\n",
    "        .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 準備\n",
    "\n",
    "* アクセスログを保存するための Iceberg テーブル `web_access_logs` を作成\n",
    "* Kafka トピックの作成\n",
    "\n",
    "### (Optional) データベースの作成\n",
    "データベースを作成していない場合、以下のセルを実行してください。既にデータベースが存在する場合は、本ステップにつきましてはスキップしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE DATABASE IF NOT EXISTS db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テーブル作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE OR REPLACE TABLE db.web_access_logs (\n",
    "    timestamp timestamp,\n",
    "    ip_address string,\n",
    "    path string,\n",
    "    status_code int,\n",
    "    user_agent string\n",
    ") USING iceberg\n",
    "PARTITIONED BY (day(timestamp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kafka トピックの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "KAFKA_TOPIC = 'web-access-logs2'\n",
    "\n",
    "\n",
    "kafka_client = KafkaAdminClient(bootstrap_servers='kafka:29092', client_id=None)\n",
    "topic = NewTopic(name=KAFKA_TOPIC, num_partitions=1, replication_factor=1)\n",
    "\n",
    "# Create a new topic\n",
    "kafka_client.create_topics(new_topics=[topic], validate_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web アクセスログを Spark Structured Streaming で Iceberg テーブルに書き込む\n",
    "### Kafka プロデューサーからデータを送信する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to produce sample web access logs to Kafka\n",
    "web_log_producer = KafkaProducer(\n",
    "    bootstrap_servers=['kafka:29092'],\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50 個のログを送信する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"/home\", \"/products\", \"/about\", \"/contact\"]\n",
    "ip_addresses = [\"192.168.1.10\", \"10.0.0.5\", \"172.16.0.3\", \"192.168.1.25\"]\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Mac OS X 14_7_2) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/18.1 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Mac OS X 14_7_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 11.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    for i in range(50):\n",
    "        log = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"ip_address\": random.choice(ip_addresses),\n",
    "            \"path\": random.choice(paths),\n",
    "            \"status_code\": random.choice([200, 200, 200, 404, 500]),\n",
    "            \"user_agent\": random.choice(user_agents)\n",
    "        }\n",
    "        web_log_producer.send(KAFKA_TOPIC, log)\n",
    "        time.sleep(0.5)\n",
    "    print(\"Completed to send 50 messages\")\n",
    "except Exception as e:\n",
    "    raise e\n",
    "finally:\n",
    "    web_log_producer.flush()\n",
    "    web_log_producer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured Streaming で Kafka からデータを取得する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from Kafka\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", KAFKA_TOPIC) \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define initial schema\n",
    "initial_schema = StructType([\n",
    "    StructField(\"timestamp\", StringType(), False),\n",
    "    StructField(\"ip_address\", StringType(), False),\n",
    "    StructField(\"path\", StringType(), False),\n",
    "    StructField(\"status_code\", IntegerType(), False),\n",
    "    StructField(\"user_agent\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse JSON and apply schema\n",
    "df_processed = df.select(\n",
    "    from_json(col(\"value\").cast(\"string\"), initial_schema).alias(\"data\")\n",
    ").select(\n",
    "    to_timestamp(col(\"data.timestamp\")).alias(\"timestamp\"),\n",
    "    col(\"data.ip_address\"),\n",
    "    col(\"data.path\"),\n",
    "    col(\"data.status_code\"),\n",
    "    col(\"data.user_agent\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iceberg テーブルにデータを書き込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Iceberg\n",
    "sq = df_processed.writeStream \\\n",
    "    .format(\"iceberg\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/iceberg-checkpoint/web-logs_tomtan\") \\\n",
    "    .toTable(\"db.web_access_logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 現在のアクセスログを集計する\n",
    "Kafka から読み取り、Iceberg テーブルに書き込んだデータを試しに読んでみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT count(*) as cnt FROM db.web_access_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT count(*) as cnt FROM db.web_access_logs\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT path, count(*) as access_cnt \n",
    "FROM db.web_access_logs\n",
    "GROUP BY path\n",
    "ORDER BY access_cnt DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT path, count(*) as access_cnt \n",
    "FROM db.web_access_logs\n",
    "GROUP BY path\n",
    "ORDER BY access_cnt DESC\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次のセクションでスキーマ変更が発生する前提での書き込み方法に切り替えるため、一度ストリーミングアプリケーションを停止します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## スキーマ進化したウェブアクセスログを書き込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_log_producer = KafkaProducer(\n",
    "    bootstrap_servers=['kafka:29092'],\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ウェブアプリケーション側の仕様変更によりスキーマが変更されたログが 50 メッセージ送信される"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"/home\", \"/products\", \"/about\", \"/contact\"]\n",
    "ip_addresses = [\"192.168.1.10\", \"10.0.0.5\", \"172.16.0.3\", \"192.168.1.25\"]\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Mac OS X 14_7_2) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/18.1 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Mac OS X 14_7_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 11.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0\"\n",
    "]\n",
    "user_ids = [\"user123\", \"user456\", \"user789\"]\n",
    "\n",
    "try:\n",
    "    for i in range(50):\n",
    "        log = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"ip_address\": random.choice(ip_addresses),\n",
    "            \"path\": random.choice(paths),\n",
    "            \"status_code\": random.choice([200, 200, 200, 404, 500]),\n",
    "            \"user_agent\": random.choice(user_agents),\n",
    "            \"response_time_ms\": random.randint(50, 2000),  # 追加されたカラム\n",
    "            \"user_id\": random.choice(user_ids)  # 追加されたカラム\n",
    "        }\n",
    "        web_log_producer.send(KAFKA_TOPIC, log)\n",
    "        time.sleep(0.5)\n",
    "    print(\"Completed to send 50 messages\")\n",
    "except Exception as e:\n",
    "    raise e\n",
    "finally:\n",
    "    web_log_producer.flush()\n",
    "    web_log_producer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### スキーマ進化したデータを読み込み Iceberg テーブルのスキーマを動的に変更しながら書き込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "ALTER TABLE db.web_access_logs SET TBLPROPERTIES('write.spark.accept-any-schema'='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_schema = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", KAFKA_TOPIC) \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema inference をしながら、新たにデータを書き込む\n",
    "def process_evolving_schema_logs(batch_df, batch_id):\n",
    "    if batch_df.isEmpty():\n",
    "        return\n",
    "\n",
    "    def _process_kafka_value_for_schema(kafka_value_str):\n",
    "        \"\"\"Process Kafka value string and infer schema\"\"\"\n",
    "        # Parse JSON string to dictionary\n",
    "        parsed_dict = json.loads(kafka_value_str)\n",
    "        return parsed_dict\n",
    "    \n",
    "    def _get_inferred_schema(df_str):\n",
    "        \"\"\"Infer schema from JSON records\"\"\"\n",
    "        # Collect all JSON strings from the dataframe\n",
    "        json_strings = [row.json_str for row in df_str.collect()]\n",
    "        \n",
    "        # Process each Kafka value string to dictionary\n",
    "        dict_records = [_process_kafka_value_for_schema(json_string) for json_string in json_strings]\n",
    "        \n",
    "        # Create RDD from dictionaries and infer schema\n",
    "        df_schema = spark.read.option('inferSchema', True).json(\n",
    "            spark.sparkContext.parallelize([json.dumps(r) for r in dict_records])\n",
    "        )\n",
    "        return df_schema.schema\n",
    "\n",
    "    # Parse all records with the inferred schema\n",
    "    df_str = batch_df.select(col(\"value\").cast(\"string\").alias(\"json_str\"))\n",
    "    df_new_schema = batch_df.select(\n",
    "        from_json(col(\"value\").cast(\"string\"), _get_inferred_schema(df_str)).alias(\"data\")\n",
    "    ).select(\"data.*\")\n",
    "\n",
    "    if \"timestamp\" in df_new_schema.columns:\n",
    "        df_new_schema = df_new_schema.withColumn(\"timestamp\", to_timestamp(col(\"timestamp\")))\n",
    "\n",
    "    # Write with schema merge enabled\n",
    "    write_options = {\n",
    "        \"merge-schema\": \"true\",  # スキーママージを有効にする\n",
    "        \"check-ordering\": \"false\"  # カラム順番のチェックを無効化する\n",
    "    }\n",
    "\n",
    "    # Write to Iceberg table\n",
    "    df_new_schema.writeTo(\"my_catalog.db.web_access_logs\").options(**write_options).append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_new_schema = df_new_schema.writeStream \\\n",
    "    .foreachBatch(process_evolving_schema_logs) \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/iceberg-checkpoint/web-logs_tomtan\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テーブルスキーマを確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "DESCRIBE db.web_access_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM db.web_access_logs WHERE user_id IS NOT NULL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### スキーマ進化したデータを読み込み Iceberg テーブルのスキーマを動的に変更しながら書き込む Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_log_producer = KafkaProducer(\n",
    "    bootstrap_servers=['kafka:29092'],\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "paths = [\"/home\", \"/products\", \"/about\", \"/contact\"]\n",
    "ip_addresses = [\"192.168.1.10\", \"10.0.0.5\", \"172.16.0.3\", \"192.168.1.25\"]\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Mac OS X 14_7_2) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/18.1 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Mac OS X 14_7_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 11.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0\"\n",
    "]\n",
    "user_ids = [\"user123\", \"user456\", \"user789\"]\n",
    "device_type = [\"mobile\", \"desktop\", \"tablet\"] # 新たにアクセス元のデバイスタイプが追加される\n",
    "\n",
    "try:\n",
    "    for i in range(50):\n",
    "        log = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"ip_address\": random.choice(ip_addresses),\n",
    "            \"path\": random.choice(paths),\n",
    "            \"status_code\": random.choice([200, 200, 200, 404, 500]),\n",
    "            \"user_agent\": random.choice(user_agents),\n",
    "            \"response_time_ms\": random.randint(50, 2000),  \n",
    "            \"user_id\": random.choice(user_ids),\n",
    "            \"device_type\": random.choice(device_type) # 新たに追加されたカラム\n",
    "        }\n",
    "        web_log_producer.send(KAFKA_TOPIC, log)\n",
    "        time.sleep(0.5)\n",
    "    print(\"Completed to send 50 messages\")\n",
    "except Exception as e:\n",
    "    raise e\n",
    "finally:\n",
    "    web_log_producer.flush()\n",
    "    web_log_producer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "スキーマ確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "DESCRIBE db.web_access_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DESCRIBE db.web_access_logs\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 集計クエリを実行する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT count(*) as total_count FROM db.web_access_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT path, AVG(response_time_ms) as avg_response_time_ms\n",
    "FROM db.web_access_logs\n",
    "WHERE response_time_ms IS NOT NULL\n",
    "GROUP BY path \n",
    "ORDER BY avg_response_time_ms DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT user_id, device_type, count(*) as access_cnt \n",
    "FROM db.web_access_logs\n",
    "WHERE user_id IS NOT NULL AND device_type IS NOT NULL\n",
    "GROUP BY user_id , device_type\n",
    "ORDER BY user_id, access_cnt DESC"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
