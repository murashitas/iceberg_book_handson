{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第 10 章: ストリーミング処理とスキーマ・パーティション進化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "CATALOG = \"my_catalog\"\n",
    "CATALOG_URL = \"http://server:8181/\"\n",
    "S3_ENDPOINT = \"http://minio:9000\"\n",
    "SPARK_VERSION = pyspark.__version__\n",
    "SPARK_MINOR_VERSION = '.'.join(SPARK_VERSION.split('.')[:2])\n",
    "ICEBERG_VERSION = \"1.8.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, current_timestamp, to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, LongType\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .config(\"spark.jars.packages\", \n",
    "                f\"org.apache.iceberg:iceberg-spark-runtime-{SPARK_MINOR_VERSION}_2.12:{ICEBERG_VERSION},org.apache.iceberg:iceberg-aws-bundle:{ICEBERG_VERSION},org.apache.spark:spark-sql-kafka-0-10_2.12:{SPARK_VERSION}\")\n",
    "        .config(f\"spark.sql.catalog.{CATALOG}\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "        .config(f\"spark.sql.catalog.{CATALOG}.type\", \"rest\")\n",
    "        .config(f\"spark.sql.catalog.{CATALOG}.uri\", CATALOG_URL)\n",
    "        .config(f\"spark.sql.catalog.{CATALOG}.s3.endpoint\", S3_ENDPOINT)\n",
    "        .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "        .config(\"spark.sql.defaultCatalog\", \"my_catalog\")\n",
    "        .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 準備\n",
    "\n",
    "* アクセスログを保存するための Iceberg テーブル `web_access_logs` を作成\n",
    "* Kafka トピックの作成\n",
    "\n",
    "### (Optional) データベースの作成\n",
    "データベースを作成していない場合、以下のセルを実行してください。既にデータベースが存在する場合は、本ステップにつきましてはスキップしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE DATABASE IF NOT EXISTS db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テーブル作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE TABLE db.web_access_logs (\n",
    "    timestamp TIMESTAMP,\n",
    "    ip_address STRING,\n",
    "    path STRING,\n",
    "    status_code INT,\n",
    "    user_agent STRING\n",
    ") USING iceberg\n",
    "PARTITIONED BY (day(timestamp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kafka トピックの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "from kafka.errors import TopicAlreadyExistsError\n",
    "KAFKA_TOPIC = 'web-access-logs'\n",
    "\n",
    "\n",
    "kafka_client = KafkaAdminClient(bootstrap_servers='kafka:29092', client_id=None)\n",
    "topic = NewTopic(name=KAFKA_TOPIC, num_partitions=1, replication_factor=1)\n",
    "\n",
    "# Create a new topic\n",
    "kafka_client.create_topics(new_topics=[topic], validate_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web アクセスログを Spark Structured Streaming で Iceberg テーブルに書き込む\n",
    "### Kafka プロデューサーからデータを送信する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to produce sample web access logs to Kafka\n",
    "web_log_producer = KafkaProducer(\n",
    "    bootstrap_servers=['kafka:29092'],\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20 個のログを送信する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"/home\", \"/products\", \"/about\", \"/contact\", \"/api/users\", \"/api/products\"]\n",
    "ip_addresses = [\"192.168.1.10\", \"10.0.0.5\", \"172.16.0.3\", \"192.168.1.25\"]\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    for i in range(20):\n",
    "        log = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"ip_address\": random.choice(ip_addresses),\n",
    "            \"path\": random.choice(paths),\n",
    "            \"status_code\": random.choice([200, 200, 200, 404, 500]),\n",
    "            \"user_agent\": random.choice(user_agents)\n",
    "        }\n",
    "        web_log_producer.send(KAFKA_TOPIC, log)\n",
    "        print(f\"Sent: {log}\")\n",
    "        time.sleep(0.5)\n",
    "except Exception as e:\n",
    "    raise e\n",
    "finally:\n",
    "    web_log_producer.flush()\n",
    "    web_log_producer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured Streaming で Kafka からデータを取得する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from Kafka\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", KAFKA_TOPIC) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define initial schema\n",
    "initial_schema = StructType([\n",
    "    StructField(\"timestamp\", StringType(), False),\n",
    "    StructField(\"ip_address\", StringType(), False),\n",
    "    StructField(\"path\", StringType(), False),\n",
    "    StructField(\"status_code\", IntegerType(), False),\n",
    "    StructField(\"user_agent\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse JSON and apply schema\n",
    "df_processed = df.select(\n",
    "    from_json(col(\"value\").cast(\"string\"), initial_schema).alias(\"data\")\n",
    ").select(\n",
    "    to_timestamp(col(\"data.timestamp\")).alias(\"timestamp\"),\n",
    "    col(\"data.ip_address\"),\n",
    "    col(\"data.path\"),\n",
    "    col(\"data.status_code\"),\n",
    "    col(\"data.user_agent\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iceberg テーブルにデータを書き込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Iceberg\n",
    "sq = df_processed.writeStream \\\n",
    "    .format(\"iceberg\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/iceberg-checkpoint/web-logs-v1\") \\\n",
    "    .option(\"fanout-enabled\", \"true\") \\\n",
    "    .toTable(\"db.web_access_logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 現在のアクセスログを集計する\n",
    "Kafka から読み取り、Iceberg テーブルに書き込んだデータを試しに読んでみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT count(*) FROM db.web_access_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT ip_address, status_code, path, count(*) as cnt FROM db.web_access_logs\n",
    "GROUP BY ip_address, status_code, path\n",
    "ORDER BY ip_address, status_code, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data\n",
    "spark.sql(\"SELECT * FROM db.web_access_logs ORDER BY timestamp DESC LIMIT 5\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次のセクションでスキーマ変更が発生する前提での書き込み方法に切り替えるため、一度ストリーミングアプリケーションを停止します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## スキーマ進化したウェブアクセスログを書き込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_log_producer = KafkaProducer(\n",
    "    bootstrap_servers=['kafka:29092'],\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ウェブアプリケーション側の仕様変更によりスキーマが変更されたログが 30 メッセージ送信される"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"/home\", \"/products\", \"/about\", \"/contact\", \"/api/users\", \"/api/products\"]\n",
    "ip_addresses = [\"192.168.1.10\", \"10.0.0.5\", \"172.16.0.3\", \"192.168.1.25\"]\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\"\n",
    "]\n",
    "user_ids = [\"user123\", \"user456\", \"user789\", None]\n",
    "\n",
    "try:\n",
    "    for i in range(30):\n",
    "        log = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"ip_address\": random.choice(ip_addresses),\n",
    "            \"path\": random.choice(paths),\n",
    "            \"status_code\": random.choice([200, 200, 200, 404, 500]),\n",
    "            \"user_agent\": random.choice(user_agents),\n",
    "            \"response_time_ms\": random.randint(50, 2000),  # 追加されたカラム\n",
    "            \"user_id\": random.choice(user_ids)  # 追加されたカラム\n",
    "        }\n",
    "        producer.send(KAFKA_TOPIC, log)\n",
    "        print(f\"Sent log with new schema: {log}\")\n",
    "        time.sleep(0.5)\n",
    "except Exception as e:\n",
    "    raise e\n",
    "finally:\n",
    "    web_log_producer.flush()\n",
    "    web_log_producer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### スキーマ進化したデータを読み込み Iceberg テーブルのスキーマを動的に変更しながら書き込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_schema = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", KAFKA_TOPIC) \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema inference をしながら、新たにデータを書き込む\n",
    "def process_evolving_schema_logs(batch_df, batch_id):\n",
    "    def get_inferred_schema(df_str):\n",
    "        \"\"\"Infer schema from JSON records\"\"\"\n",
    "        json_records = df_str.collect()     \n",
    "        parsed_records = []\n",
    "        for row in json_records:\n",
    "            try:\n",
    "                parsed_records.append(json.loads(row.json_string))\n",
    "            except Exception as e:\n",
    "                raise e\n",
    "        df_schema = spark.read.option('inferSchema', True).json(spark.sparkContext.parallelize([json.dumps(r) for r in parsed_records]))\n",
    "        return df_schema.schema\n",
    "                \n",
    "   # Extract the JSON string values from Kafka messages\n",
    "   df_str = batch_df.select(col(\"value\").cast(\"string\").alias(\"json_str\"))\n",
    "\n",
    "   # Parse all records with the inferred schema\n",
    "   df_new_schema = batch_df.select(\n",
    "       from_json(col(\"value\").cast(\"string\"), get_inferred_schema(df_str)).alias(\"data\")\n",
    "   ).select(\"data.*\")\n",
    "\n",
    "   # Add timestamp conversion if timestamp field exists\n",
    "   if \"timestamp\" in df_new_schema.columns:\n",
    "       df_new_schema = df_new_schema.withColumn(\"timestamp\", to_timestamp(col(\"timestamp\")))\n",
    "\n",
    "\n",
    "   # Write with schema merge enabled\n",
    "   write_options = {\n",
    "       \"merge-schema\": \"true\",  # スキーママージを有効にする\n",
    "       \"check-ordering\": \"false\"  # Allow column reordering\n",
    "   }\n",
    "    df_new_schema.printSchema()\n",
    "    df.show()\n",
    "       \n",
    "   # Write to Iceberg table\n",
    "   # df_new_schema.writeTo(\"db.web_access_logs\").options(**write_options).append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start streaming with foreachBatch\n",
    "sq_new_schema = df_new_schema.writeStream \\\n",
    "    .foreachBatch(process_evolving_schema_logs) \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/iceberg-checkpoint/web-logs-v2\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テーブルスキーマを確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "DESCRIBE db.web_access_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 集計クエリを実行する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT count(*) as total FROM db.web_access_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各ユーザーからのアクセス状況。これを基に、各ユーザーにレコメンデーションができる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT ip_address, status_code, path, count(*) as cnt FROM db.web_access_logs\n",
    "GROUP BY user_id, status_code, path\n",
    "ORDER BY user_id, status_code, path\n",
    "WHERE user_id IS NOT NULL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "平均レスポンス時間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT path, AVG(response_time_ms) as avg_response_time_ms\n",
    "FROM db.web_access_logs\n",
    "GROUP BY path ORDER BY avg_response_time_ms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
