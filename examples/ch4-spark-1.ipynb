{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第 4 章: Apache Spark - 1\n",
    "\n",
    "本ノートブックでは、「Spark でデータ処理を実行する」節で紹介されている例を実行できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark でデータ処理を実行する\n",
    "### 初期設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINIO_ACCESS_KEY = \"admin\"\n",
    "MINIO_SECRET_KEY = \"password\"\n",
    "S3_ENDPOINT = \"http://minio:9000\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. SparkSession オブジェクトを初期化する\n",
    "\n",
    "以下のセルを実行して SparkSession オブジェクトを初期化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.4,org.apache.hadoop:hadoop-client:3.2.4\")\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", MINIO_ACCESS_KEY)\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", MINIO_SECRET_KEY)\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", S3_ENDPOINT)\n",
    "        .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) データベースの作成\n",
    "データベースを作成していない場合、以下のセルを実行してください。既にデータベースが存在する場合は、本ステップにつきましてはスキップしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE DATABASE IF NOT EXISTS db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. テーブルを作成する\n",
    "\n",
    "`sales_data` テーブルを作成します。本テーブルには JSON 形式でデータが保存されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE TABLE db.sales_data (\n",
    "    product_name string,\n",
    "    price decimal(10, 2),\n",
    "    customer_id bigint,\n",
    "    order_id string,\n",
    "    datetime timestamp,\n",
    "    category string\n",
    ") USING JSON\n",
    "LOCATION 's3a://amzn-s3-demo-bucket/spark/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に `sales_analysis` テーブルを作成します。本テーブルには Parquet 形式でデータが保存されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE TABLE db.sales_analysis (\n",
    "  category string,\n",
    "  total_sales decimal(20,2),\n",
    "  count_by_year int,\n",
    "  year int\n",
    ") USING parquet\n",
    "LOCATION 's3a://amzn-s3-demo-bucket/spark/analysis'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 読み込んだデータを基に集計処理を実行し、データを別のテーブルへ書き込む\n",
    "\n",
    "`sales_data` におけるデータを読み込み、集計処理を実行して `sales_analysis` に Parquet 形式で結果を書き込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "INSERT INTO db.sales_analysis\n",
    "SELECT\n",
    "  category,\n",
    "  sum(price) AS total_sales,\n",
    "  count(*) AS count_by_year,\n",
    "  year(datetime) AS year\n",
    "FROM db.sales_data\n",
    "GROUP BY category, year(datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後に書き込んだデータを読み出します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM db.sales_analysis ORDER BY year DESC, category ASC;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Spark DataFrame で集計処理を行う方法\n",
    "\n",
    "以下のセルを実行し、Spark DataFrame を利用して同様の集計処理を行うことができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, count, round, year\n",
    "\n",
    "df.groupBy(\"category\", year(\"datetime\").alias(\"year\")) \\\n",
    "    .agg(round(sum(\"price\")).alias(\"total_sales\"), \n",
    "                  count(\"*\").alias(\"count_by_year\")) \\\n",
    "    .select(\"category\", \"total_sales\", \"count_by_year\", \"year\").orderBy('year', ascending=False).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
