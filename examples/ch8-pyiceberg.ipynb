{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45be626b",
   "metadata": {},
   "source": [
    "# 第 8 章: PyIceberg 〜軽量な環境における Iceberg 〜\n",
    "\n",
    "PyIceberg は Apache Iceberg の公式 Python クライアントライブラリであり、分散処理環境や Java の実行環境を必要とせず、Python 環境から Iceberg テーブルを直接操作することを可能にします。本ハンズオンでは、PyIceberg の基本的な使い方を実際に試しながら学んでいきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2253ad26-e58d-47de-9d98-009bf2256043",
   "metadata": {},
   "source": [
    "## PyIceberg を使った基本的なテーブル操作を体験する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dec5a9-4c3d-4835-a65e-e62ec1a391b4",
   "metadata": {},
   "source": [
    "ここでは、PyIceberg を使ったシンプルなテーブル操作の流れを概観してみましょう。天候の観測データを例に、インストールから設定、テーブル作成、データの追加・読み取り・更新までの基本的な操作を確認します。各要素の詳細な説明は後続のセクションで行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df499f84-d05f-4a15-bbd5-7541070f34ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyIceberg をインストール\n",
    "!pip install 'pyiceberg' -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3c73ac-5ff4-4dbf-a128-cd2856d5f292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iceberg カタログへの接続設定\n",
    "from pyiceberg.catalog import load_catalog\n",
    "\n",
    "CATALOG_URL     = \"http://server:8181\"\n",
    "DEMO_WAREHOUSE  = \"s3://amzn-s3-demo-bucket\"\n",
    "\n",
    "props = {\n",
    "    \"type\": \"rest\",\n",
    "    \"uri\": CATALOG_URL,\n",
    "    \"warehouse\": DEMO_WAREHOUSE,\n",
    "    \"s3.endpoint\": \"http://minio:9000\",\n",
    "}\n",
    "\n",
    "catalog = load_catalog(**props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc486f69-939b-4cd1-8e95-2a907c07ff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 名前空間の作成\n",
    "namespace = \"pyIceberg\"\n",
    "if not catalog.namespace_exists(\"pyIceberg\"):\n",
    "    catalog.create_namespace(\"pyIceberg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d1e87e-be7d-4ed0-8e5c-4a28d31a2aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# テーブルのスキーマ定義\n",
    "from pyiceberg.schema import Schema\n",
    "from pyiceberg.types import (\n",
    "    TimestampType,\n",
    "    DoubleType,\n",
    "    StringType,\n",
    "    NestedField,\n",
    "    FloatType\n",
    ")\n",
    "\n",
    "weather_schema = Schema(\n",
    "    NestedField(field_id=1, name=\"observation_time\", field_type=TimestampType(), required=False),\n",
    "    NestedField(field_id=2, name=\"station_id\", field_type=StringType(), required=False),\n",
    "    NestedField(field_id=3, name=\"temperature\", field_type=DoubleType(), required=False),\n",
    "    NestedField(field_id=4, name=\"humidity\", field_type=FloatType(), required=False),\n",
    "    NestedField(field_id=5, name=\"wind_speed\", field_type=FloatType(), required=False),\n",
    "    NestedField(field_id=6, name=\"precipitation\", field_type=FloatType(), required=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0c24a2-d19b-4bf6-9aaa-95979e331ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"observations\"\n",
    "table_identifier = f\"{namespace}.{table_name}\"\n",
    "\n",
    "# テーブル作成\n",
    "if catalog.table_exists(table_identifier):\n",
    "    catalog.purge_table(table_identifier)\n",
    "\n",
    "table = catalog.create_table(\n",
    "    identifier=table_identifier,\n",
    "    schema=weather_schema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3c0f40-a9c0-49c6-96a7-d695e78331f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# サンプルデータの作成\n",
    "data = pa.Table.from_pylist([\n",
    "    {\n",
    "        \"observation_time\": datetime(2023, 7, 1, 12, 0, 0),\n",
    "        \"station_id\": \"TOKYO_001\",\n",
    "        \"temperature\": 28.5,\n",
    "        \"humidity\": np.float32(65.0),\n",
    "        \"wind_speed\": np.float32(3.2),\n",
    "        \"precipitation\": np.float32(0.0)\n",
    "    },\n",
    "    {\n",
    "        \"observation_time\": datetime(2023, 7, 1, 12, 0, 0),\n",
    "        \"station_id\": \"OSAKA_002\",\n",
    "        \"temperature\": 30.2,\n",
    "        \"humidity\": np.float32(70.5),\n",
    "        \"wind_speed\": np.float32(2.1),\n",
    "        \"precipitation\": np.float32(0.0)\n",
    "    }\n",
    "])\n",
    "\n",
    "# テーブルにデータを追加\n",
    "table.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab900e35-944d-414e-b518-516020c43720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ読み取り\n",
    "scan = table.scan()\n",
    "scan.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5be8645-8c83-46d9-a0dd-c158ba66d547",
   "metadata": {},
   "source": [
    "以上が PyIceberg を使った基本的なテーブル操作の流れです。このシンプルな例から、PyIceberg を使うことで Python 環境から直接 Iceberg テーブルを操作するイメージが掴めたと思います。ここからは、より詳細な機能とそれらの活用方法について説明します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98ef458-5edb-47c6-a49c-f6f044cc640b",
   "metadata": {},
   "source": [
    "## PyIceberg の基本的な使い方"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e0b7ce-013c-40f5-8e87-efbc92f8e2d0",
   "metadata": {},
   "source": [
    "### PyIceberg のインストールと設定\n",
    "\n",
    "まずは PyIceberg をインストールします。このハンズオン環境では既にインストールされていますが、通常は以下のコマンドでインストールします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97635b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'pyiceberg' -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039b9928",
   "metadata": {},
   "source": [
    "加えて、PyIceberg には特定のデータソースやデータ処理ツールとの統合に使用する追加の依存関係​が用意されています。例えば、テーブルのデータを Pandas で扱い、Iceberg カタログとして AWS Glue Data Catalog を使用する場合には `pandas` と `glue` の依存関係を追加します。インストール時にこれらを指定するには、パッケージ名の後に以下のように括弧 [] で指定します：\n",
    "\n",
    "| Key             | 説明                                                                                                      |\n",
    "|-----------------|-----------------------------------------------------------------------------------------------------------|\n",
    "| hive            | Hive メタストアをサポート                                                                                 |\n",
    "| hive-kerberos   | Kerberos 環境下での Hive メタストアをサポート                                                               |\n",
    "| glue            | AWS Glue をサポート                                                                                       |\n",
    "| dynamodb        | Amazon DynamoDB をサポート                                                                                   |\n",
    "| sql-postgres    | PostgreSQL をバックエンドとした SQL カタログをサポート                                                      |\n",
    "| sql-sqlite      | SQLite をバックエンドとした SQL カタログをサポート                                                          |\n",
    "| pyarrow         | オブジェクトストアとやりとりするための FileIO 実装として PyArrow を利用                                       |\n",
    "| pandas          | PyArrow と Pandas をインストール                                                                      |\n",
    "| duckdb          | PyArrow と DuckDB をインストール                                                                      |\n",
    "| ray             | PyArrow、Pandas、Rayをインストール                                                                        |\n",
    "| daft            | Daft をインストール                                                                                       |\n",
    "| polars          | Polars をインストール                                                                                    |\n",
    "| s3fs            | オブジェクトストアとやりとりするための FileIO 実装として S3FS を利用                                          |\n",
    "| adlfs           | オブジェクトストアとやりとりするための FileIO 実装として ADLFS を利用                                         |\n",
    "| snappy          | snappy による Avro 圧縮をサポート                                                                           |\n",
    "| gcsfs           | オブジェクトストアとやりとりするための FileIO 実装として GCSFS を利用                                         |\n",
    "| rest-sigv4      | REST カタログのための AWS SIGv4 認証ヘッダ生成をサポート                                                   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a003eb7a-a45f-47e1-a51c-b2f76bedd859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandasの依存関係をインストールする例\n",
    "!pip install 'pyiceberg[pandas]' -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c161b3f2",
   "metadata": {},
   "source": [
    "### Iceberg カタログへの接続\n",
    "\n",
    "PyIceberg で Iceberg テーブルを操作するには、まずテーブルを管理するカタログへの接続を設定します。このハンズオン環境では REST カタログを使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87e386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iceberg カタログへの接続設定\n",
    "from pyiceberg.catalog import load_catalog\n",
    "\n",
    "CATALOG_URL     = \"http://server:8181\"\n",
    "DEMO_WAREHOUSE  = \"s3://amzn-s3-demo-bucket\"\n",
    "\n",
    "props = {\n",
    "    \"type\": \"rest\",\n",
    "    \"uri\": CATALOG_URL,\n",
    "    \"warehouse\": DEMO_WAREHOUSE,\n",
    "    \"s3.endpoint\": \"http://minio:9000\",\n",
    "}\n",
    "\n",
    "catalog = load_catalog(**props)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e2db6e",
   "metadata": {},
   "source": [
    "### 名前空間の作成と確認\n",
    "\n",
    "テーブルを作成する前に、名前空間（namespace）を作成します。名前空間は、テーブルを論理的にグループ化するためのものです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128a93d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 既存の名前空間を確認\n",
    "existing_namespaces = catalog.list_namespaces()\n",
    "print(\"既存の名前空間:\")\n",
    "for namespace in existing_namespaces:\n",
    "    print(f\"- {'.'.join(namespace)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021d33e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 名前空間の作成\n",
    "namespace = \"pyIceberg\"\n",
    "if not catalog.namespace_exists(namespace):\n",
    "    catalog.create_namespace(namespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53edaf4",
   "metadata": {},
   "source": [
    "### テーブルの作成\n",
    "ここでは、サンプルとなる天候の観測データを格納するテーブルを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fbf2c0-62ca-406f-8930-8a87228597b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"observations\"\n",
    "table_identifier = f\"{namespace}.{table_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfaf15e-e987-4e62-b7cb-5a6f895dde44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 対象のテーブルが既に存在する場合は削除\n",
    "if catalog.table_exists(table_identifier):\n",
    "    catalog.purge_table(table_identifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e276975b-9c0d-410d-8e1a-134d03f5df4c",
   "metadata": {},
   "source": [
    "テーブルを作成するために、スキーマを定義します。PyIceberg では、スキーマを定義するために `Schema` クラスを使用します。  \n",
    "テーブルの列は、 `NestedField` クラスを使用して定義します。`NestedField` クラスは、以下のパラメータを持ちます： \n",
    "\n",
    "- field_id: 列の一意な識別子\n",
    "- name: 列の名前\n",
    "- field_type: 列のデータ型\n",
    "- required: 値が必須かどうか"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c3d4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyiceberg.schema import Schema\n",
    "from pyiceberg.types import (\n",
    "    TimestampType,\n",
    "    DoubleType,\n",
    "    StringType,\n",
    "    NestedField,\n",
    "    FloatType\n",
    ")\n",
    "\n",
    "# テーブルのスキーマ定義\n",
    "weather_schema = Schema(\n",
    "    NestedField(field_id=1, name=\"observation_time\", field_type=TimestampType(), required=False),\n",
    "    NestedField(field_id=2, name=\"station_id\", field_type=StringType(), required=False),\n",
    "    NestedField(field_id=3, name=\"temperature\", field_type=DoubleType(), required=False),\n",
    "    NestedField(field_id=4, name=\"humidity\", field_type=FloatType(), required=False),\n",
    "    NestedField(field_id=5, name=\"wind_speed\", field_type=FloatType(), required=False),\n",
    "    NestedField(field_id=6, name=\"precipitation\", field_type=FloatType(), required=False)\n",
    ")\n",
    "\n",
    "# スキーマの確認\n",
    "print(weather_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8960d4a6",
   "metadata": {},
   "source": [
    "スキーマとパーティション仕様を定義したら、テーブルを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa84d740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# テーブルの作成\n",
    "table = catalog.create_table(\n",
    "    identifier=table_identifier,\n",
    "    schema=weather_schema\n",
    ")\n",
    "print(f\"テーブル '{table_identifier}' を作成しました\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a60889-f4e2-4d67-810b-b8dcd3f83886",
   "metadata": {},
   "source": [
    "### PyArrow スキーマを活用したテーブル作成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a118cf93-41e0-41d0-b174-47a167bd1712",
   "metadata": {},
   "source": [
    "テーブルのスキーマを定義する際の別の方法として、 PyIceberg では、 PyArrow のスキーマを直接使用してテーブルを作成することができます。PyArrow スキーマを使用することで、スキーマ定義の手間を省き、既存のデータ構造との一貫性を保ちながらテーブルを作成できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d615b223-c538-4e70-818d-34e2dfa29d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "table_name = \"observations\"\n",
    "table_identifier = f\"{namespace}.{table_name}\"\n",
    "\n",
    "# PyArrowスキーマの定義\n",
    "arrow_schema = pa.schema([\n",
    "    pa.field(\"observation_time\", pa.timestamp('us'), nullable=False),\n",
    "    pa.field(\"station_id\", pa.string(), nullable=False),\n",
    "    pa.field(\"temperature\", pa.float64(), nullable=False),\n",
    "    pa.field(\"humidity\", pa.float32(), nullable=False),\n",
    "    pa.field(\"wind_speed\", pa.float32(), nullable=False),\n",
    "    pa.field(\"precipitation\", pa.float32(), nullable=False)\n",
    "])\n",
    "\n",
    "# PyArrowスキーマを使用してテーブルを作成\n",
    "if catalog.table_exists(table_identifier):\n",
    "    catalog.purge_table(table_identifier)\n",
    "table = catalog.create_table(\n",
    "    identifier=table_identifier,\n",
    "    schema=arrow_schema\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bdc05c-7599-4255-b82a-4689881080f6",
   "metadata": {},
   "source": [
    "この使い方は、既存のデータファイルからスキーマを抽出してそのままテーブル作成に利用するシナリオで特に価値を発揮します。例えば、ローカルに保存されたParquetファイルを読み込み、そのスキーマを使用してIcebergテーブルを作成する場合を考えてみましょう。以下のように、PyArrow を使用して Parquet ファイルを読み込み、そのスキーマをそのまま Iceberg テーブルの作成に利用できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf572b0-9b3d-40eb-8c71-542786b50e09",
   "metadata": {},
   "source": [
    "実験用に適当な Parquet ファイルを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e96078-9823-4188-9b73-1d96c03bb4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "\n",
    "def generate_simple_weather_data(num_records=30):\n",
    "    data = {\n",
    "        \"temperature\": [round(20 + np.random.normal(0, 5), 1) for _ in range(num_records)],\n",
    "        \"humidity\": [round(60 + np.random.normal(0, 10), 1) for _ in range(num_records)],\n",
    "        \"weather_condition\": np.random.choice([\"SUNNY\", \"CLOUDY\", \"RAINY\"], size=num_records)\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# サンプルデータを生成\n",
    "output_path = \"weather_data.parquet\"\n",
    "weather_df = generate_simple_weather_data(30)\n",
    "\n",
    "# Parquetファイルとして保存\n",
    "weather_table = pa.Table.from_pandas(weather_df)\n",
    "pq.write_table(weather_table, output_path)\n",
    "print(f\"Parquetファイルを保存しました: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810ad910-2049-4a1c-9217-cdc8bd142dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファイルのスキーマを確認\n",
    "parquet_file = pq.ParquetFile(output_path)\n",
    "print(\"\\nParquetファイルのスキーマ:\")\n",
    "print(parquet_file.schema.to_arrow_schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7908bd-d978-479f-beeb-8af3b832d5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最初の数行を確認\n",
    "print(\"\\nデータサンプル（最初の5行）:\")\n",
    "first_rows = next(parquet_file.iter_batches(batch_size=5)).to_pandas()\n",
    "print(first_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8470474-7531-4de4-8568-950940dd6ca7",
   "metadata": {},
   "source": [
    "この Parquet ファイルを元に、Iceberg テーブルを作ってみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546ab253-587c-4c50-bd2b-17eee282c893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyiceberg\n",
    "from pyiceberg.catalog import load_catalog\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "table_name = \"observations\"\n",
    "table_identifier = f\"{namespace}.{table_name}\"\n",
    "\n",
    "# ローカルのParquetファイルを読み込む\n",
    "parquet_file = pq.ParquetFile('weather_data.parquet')\n",
    "# ファイルからスキーマを取得\n",
    "file_schema = parquet_file.schema.to_arrow_schema()\n",
    "\n",
    "# 取得したスキーマを使用してIcebergテーブルを作成\n",
    "if catalog.table_exists(table_identifier):\n",
    "    catalog.purge_table(table_identifier)\n",
    "    \n",
    "table = catalog.create_table(\n",
    "    identifier=table_identifier,\n",
    "    schema=file_schema\n",
    ")\n",
    "\n",
    "# Parquetファイルからデータを読み込み\n",
    "table_data = pq.read_table('weather_data.parquet')\n",
    "# 読み込んだデータをIcebergテーブルに追加\n",
    "table.append(table_data)\n",
    "\n",
    "print(\"Icebergテーブルの作成と読み込みが完了しました\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25199da3-6e33-4244-b198-c6ff9d419883",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0383e328-3440-4ded-b424-ab7854790ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.scan().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a268a9a9-b946-47b0-8226-2b8014541fd6",
   "metadata": {},
   "source": [
    "このアプローチの利点は、スキーマを手動で再定義する必要がなく、元のデータ構造がそのまま保持されることです。PyIceberg は PyArrow スキーマからテーブルを作成する際に、自動的に各フィールドに一意のIDを割り当てます。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93957b1-ce5b-4c8c-a26c-138c3e9edb4a",
   "metadata": {},
   "source": [
    "これは、Python のデータ分析ライブラリである Pandas や Polars などと組み合わせて使用する際にも便利です。PyArrow スキーマを使用することで、Pandas DataFrame や Polars DataFrame から直接 Iceberg テーブルを作成できるため、データの前処理や変換を行った後に、そのまま Iceberg テーブルとして保存できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31aff16b-1377-4ff1-b5da-de363b95f7da",
   "metadata": {},
   "source": [
    "例えば、Pandas DataFrame から直接 Iceberg テーブルを作成する例を見てみましょう："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a358ebc3-98ce-4324-b32e-c01561c32f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from pyiceberg.utils.config import Config \n",
    "\n",
    "table_name = \"pandas_observations\"\n",
    "table_identifier = f\"{namespace}.{table_name}\"\n",
    "\n",
    "# Pandas DataFrameの作成\n",
    "df = pd.DataFrame({\n",
    "    \"station_id\": [\"TOKYO001\", \"OSAKA002\", \"NAGOYA003\"],\n",
    "    \"temperature\": [28.5, 30.2, 27.8],\n",
    "    \"humidity\": [65.0, 70.2, 68.5],\n",
    "    \"wind_speed\": [3.2, 2.1, 4.5],\n",
    "    \"precipitation\": [0.0, 0.0, 2.5]\n",
    "})\n",
    "\n",
    "# Pandas DataFrameをPyArrow Tableに変換\n",
    "arrow_table = pa.Table.from_pandas(df)\n",
    "\n",
    "# PyArrow Tableのスキーマを使用してIcebergテーブルを作成\n",
    "if catalog.table_exists(table_identifier):\n",
    "    catalog.purge_table(table_identifier)\n",
    "table = catalog.create_table(\n",
    "    identifier=table_identifier,\n",
    "    schema=arrow_table.schema\n",
    ")\n",
    "\n",
    "# 作成したテーブルにデータを追加\n",
    "table.append(arrow_table)\n",
    "# 結果を表示\n",
    "table.scan().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea4b70a-1e79-4c4f-879f-9783eea4fe54",
   "metadata": {},
   "source": [
    "この例では、Pandas で作成したデータフレームを PyArrow テーブルに変換し、そのスキーマを使って Iceberg テーブルを作成しています。その後、同じデータを Iceberg テーブルに追加しています。このように、Pandas で前処理したデータを PyArrow に変換し、そのスキーマをそのまま使って Iceberg テーブルを作成することで、分析ワークフローをシンプルに保ちながら、Iceberg のデータ管理機能を活用できます。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a95b42-839b-4b7c-8aac-37381a560883",
   "metadata": {},
   "source": [
    "## テーブルへの書き込み"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9050d57-4884-4af1-b321-d930f1cb3dd4",
   "metadata": {},
   "source": [
    "PyIceberg では、Apache Arrow を使用してテーブルにデータを書き込めます。主な書き込み操作として、`append`（追加）と `overwrite`（上書き）の2つの方法があります。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6cb16a-41e0-4954-8de0-4816db3b5e00",
   "metadata": {},
   "source": [
    "## サンプルデータの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a236dd-883c-402a-8deb-244e75fb7286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプルテーブル作成\n",
    "from pyiceberg.schema import Schema\n",
    "from pyiceberg.types import (\n",
    "    TimestampType,\n",
    "    DoubleType,\n",
    "    StringType,\n",
    "    NestedField,\n",
    "    FloatType\n",
    ")\n",
    "\n",
    "table_name = \"observations\"\n",
    "table_identifier = f\"{namespace}.{table_name}\"\n",
    "\n",
    "weather_schema = Schema(\n",
    "    NestedField(field_id=1, name=\"observation_time\", field_type=TimestampType(), required=False),\n",
    "    NestedField(field_id=2, name=\"station_id\", field_type=StringType(), required=False),\n",
    "    NestedField(field_id=3, name=\"temperature\", field_type=DoubleType(), required=False),\n",
    "    NestedField(field_id=4, name=\"humidity\", field_type=FloatType(), required=False),\n",
    "    NestedField(field_id=5, name=\"wind_speed\", field_type=FloatType(), required=False),\n",
    "    NestedField(field_id=6, name=\"precipitation\", field_type=FloatType(), required=False)\n",
    ")\n",
    "\n",
    "\n",
    "if catalog.table_exists(table_identifier):\n",
    "    catalog.purge_table(table_identifier)\n",
    "\n",
    "table = catalog.create_table(\n",
    "    identifier=table_identifier,\n",
    "    schema=weather_schema\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647658e8-c593-4035-8932-abe0642f2515",
   "metadata": {},
   "source": [
    "### データの追加（Append）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e234e7-ee5d-46bd-8af3-456d0aae2c44",
   "metadata": {},
   "source": [
    "`append` は、テーブルにレコードを挿入する際に使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd0563a-4273-4a6c-8131-9a72c0c1d56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "data = pa.Table.from_pylist([\n",
    "    {\n",
    "        \"observation_time\": datetime(2025, 4, 28, 12, 0, 0),\n",
    "        \"station_id\": \"TOKYO001\",\n",
    "        \"temperature\": 23.5,\n",
    "        \"humidity\": np.float32(65.2),\n",
    "        \"wind_speed\": np.float32(4.3),\n",
    "        \"precipitation\": np.float32(0.0)\n",
    "    },\n",
    "    {\n",
    "        \"observation_time\": datetime(2025, 4, 28, 13, 0, 0),\n",
    "        \"station_id\": \"OSAKA002\",\n",
    "        \"temperature\": 25.3,\n",
    "        \"humidity\": np.float32(58.6),\n",
    "        \"wind_speed\": np.float32(3.7),\n",
    "        \"precipitation\": np.float32(0.0)\n",
    "    },\n",
    "    {\n",
    "        \"observation_time\": datetime(2025, 4, 28, 14, 0, 0),\n",
    "        \"station_id\": \"NAGOYA003\",\n",
    "        \"temperature\": 22.8,\n",
    "        \"humidity\": np.float32(70.5),\n",
    "        \"wind_speed\": np.float32(5.2),\n",
    "        \"precipitation\": np.float32(1.4)\n",
    "    }\n",
    "])\n",
    "\n",
    "# テーブルに追加\n",
    "table.append(data)\n",
    "table.scan().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aeedac1-610a-4c10-8f95-5ab7460a2b31",
   "metadata": {},
   "source": [
    "### データの上書き（Overwrite）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca54a8a-5ceb-4e80-b288-69eedbee73d9",
   "metadata": {},
   "source": [
    "既存データを新しいデータで置き換える場合は、`overwrite` メソッドを使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422fbeb1-f090-45a3-819a-e298a3c3364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_data = pa.Table.from_pylist([\n",
    "    {\n",
    "        \"observation_time\": datetime.fromtimestamp(1651234567000000 / 1000000),\n",
    "        \"station_id\": \"TOKYO001\",\n",
    "        \"temperature\": 26.8,\n",
    "        \"humidity\": np.float32(64.5),\n",
    "        \"wind_speed\": np.float32(3.5),\n",
    "        \"precipitation\": np.float32(0.2)\n",
    "    },\n",
    "    {\n",
    "        \"observation_time\": datetime.fromtimestamp(1651234587000000 / 1000000),\n",
    "        \"station_id\": \"OSAKA002\",\n",
    "        \"temperature\": 29.1,\n",
    "        \"humidity\": np.float32(69.0),\n",
    "        \"wind_speed\": np.float32(2.3),\n",
    "        \"precipitation\": np.float32(0.0)\n",
    "    }\n",
    "])\n",
    "\n",
    "# テーブルのデータを上書き\n",
    "table.overwrite(updated_data)\n",
    "table.scan().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5556a57f-2b24-4c46-9117-b335c10273e6",
   "metadata": {},
   "source": [
    "### 条件付き上書き（Partial Overwrite）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fcd25c-a232-4182-9486-3732291422f3",
   "metadata": {},
   "source": [
    "特定の条件に一致するデータのみを上書きするには、`overwrite_filter` パラメータを使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4646cf6-0455-4f0e-99ef-a74b56f51035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyiceberg.expressions import EqualTo\n",
    "\n",
    "# 条件付き上書き（station_idがTOKYO001のレコードのみを更新）\n",
    "updated_tokyo = pa.Table.from_pylist([\n",
    "    {\n",
    "        \"observation_time\": datetime.fromtimestamp(1651234567000000 / 1000000),\n",
    "        \"station_id\": \"TOKYO001\",\n",
    "        \"temperature\": 27.0,\n",
    "        \"humidity\": np.float32(63.0),\n",
    "        \"wind_speed\": np.float32(100.8),\n",
    "        \"precipitation\": np.float32(0.5)\n",
    "    }\n",
    "])\n",
    "\n",
    "# station_idがTOKYO001のレコードを上書き\n",
    "table.overwrite(updated_tokyo, overwrite_filter=EqualTo(\"station_id\", \"TOKYO001\"))\n",
    "table.scan().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff6f3c8-d36a-465b-8ca8-332593d65ce8",
   "metadata": {},
   "source": [
    "### 動的パーティション上書き（Dynamic Partition Overwrite）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b81939-893f-4dba-809e-cae50ccd7d76",
   "metadata": {},
   "source": [
    "パーティション化されたテーブルでは、データの一部を効率的に更新する必要がしばしば生じます。PyIceberg の `dynamic_partition_overwrite` メソッドは、データに含まれるパーティション値を自動的に検出し、該当するパーティションのみを上書きする機能を提供します。これにより、特定のパーティションのデータのみを更新できます。\n",
    "\n",
    "この機能は、日次データの更新や、特定のカテゴリのデータを修正する場合など、増分更新のシナリオで特に有用です。\n",
    "例えば、商品カテゴリごとにパーティション化された販売データテーブルで、電子機器カテゴリの価格データに誤りがあり、それを修正したい場合を考えてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9168e2a1-674e-4c4b-bf76-7dd71dde1812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# category　カラムでパーティション化されたテーブルを作成\n",
    "from pyiceberg.schema import Schema\n",
    "from pyiceberg.types import LongType, NestedField, StringType, DecimalType\n",
    "from pyiceberg.partitioning import PartitionSpec, PartitionField\n",
    "from pyiceberg.transforms import IdentityTransform\n",
    "\n",
    "table_name = \"products\"\n",
    "table_identifier = f\"{namespace}.{table_name}\"\n",
    "\n",
    "schema = Schema(\n",
    "    NestedField(1, \"product_id\", StringType(), required=False),\n",
    "    NestedField(2, \"category\", StringType(), required=False),\n",
    "    NestedField(3, \"product_name\", StringType(), required=False),\n",
    "    NestedField(4, \"price\", FloatType(), required=False),\n",
    "    NestedField(5, \"stock\", LongType(), required=False)\n",
    ")\n",
    "\n",
    "# category　カラムによるーパーティションを定義\n",
    "partition = PartitionSpec(PartitionField(source_id=2, field_id=1001, transform=IdentityTransform(), name=\"category_identity\"))\n",
    "\n",
    "if catalog.table_exists(table_identifier):\n",
    "    catalog.purge_table(table_identifier)\n",
    "table = catalog.create_table(\n",
    "    table_identifier,\n",
    "    schema=schema,\n",
    "    partition_spec=partition\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466115e0-8a97-4a56-a6cb-ffd7d7d6cd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "\n",
    "# 初期データを追加（電子機器カテゴリの価格に誤りがある）\n",
    "df = pa.Table.from_pylist([\n",
    "    {\"product_id\": \"A001\", \"category\": \"電子機器\", \"product_name\": \"スマートフォン\", \"price\": np.float32(89999.99), \"stock\": 50},\n",
    "    {\"product_id\": \"A002\", \"category\": \"電子機器\", \"product_name\": \"ノートパソコン\", \"price\": np.float32(149999.99), \"stock\": 30},\n",
    "    {\"product_id\": \"B001\", \"category\": \"家具\", \"product_name\": \"ソファ\", \"price\": np.float32(59999.99), \"stock\": 10},\n",
    "    {\"product_id\": \"B002\", \"category\": \"家具\", \"product_name\": \"ダイニングテーブル\", \"price\": np.float32(39999.99), \"stock\": 15}\n",
    "])\n",
    "table.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88e4ac6-1c5e-4723-b412-97305807b071",
   "metadata": {},
   "source": [
    "ここで、電子機器カテゴリの価格データを修正したいとします。`dynamic_partition_overwrite` メソッドを使用して、電子機器カテゴリのデータのみを更新します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d45109-aaf3-4005-8916-74ddc2289627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 電子機器カテゴリの正しい価格データを含むテーブル\n",
    "df_corrected = pa.Table.from_pylist([\n",
    "    {\"product_id\": \"A001\", \"category\": \"電子機器\", \"product_name\": \"スマートフォン\", \"price\": np.float32(\"79999.99\"), \"stock\": 50},\n",
    "    {\"product_id\": \"A002\", \"category\": \"電子機器\", \"product_name\": \"ノートパソコン\", \"price\": np.float32(\"129999.99\"), \"stock\": 30}\n",
    "])\n",
    "\n",
    "# 動的パーティション上書きを実行\n",
    "table.dynamic_partition_overwrite(df_corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a924fe17-7a27-4ff0-abb4-d99fdc24b90e",
   "metadata": {},
   "source": [
    "この操作により、「電子機器」パーティションのデータのみが新しい価格データで上書きされ、「家具」カテゴリのデータはそのまま保持されます。テーブルの内容を確認すると、以下のようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d6eabe-9687-4f4f-8dc7-bbb095343921",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.scan().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62782775-2e79-408c-b6fc-9af233813add",
   "metadata": {},
   "source": [
    "このように、`dynamic_partition_overwrite` を使用することで、テーブル全体を再作成することなく、特定のパーティションのデータのみを効率的に更新できます。これは大規模なデータセットを扱う際に特に重要で、更新が必要なデータのみを処理することでリソース使用量を削減し、処理時間を短縮できます。  \n",
    "\n",
    "また、複数のパーティションを同時に更新することも可能です。更新用のデータに複数のパーティション値が含まれている場合、それらのパーティションすべてが自動的に更新対象となります。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9cd6a2-92e3-4f99-b29f-fd3a429c3407",
   "metadata": {},
   "source": [
    "## アップサート操作（Upsert）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30c9838-7cd5-4af8-939b-3c70138389d8",
   "metadata": {},
   "source": [
    "PyIceberg は、既存データの更新と新規データの挿入を1回の操作で行う「アップサート」（upsert）機能をサポートしています。この機能は、テーブルのスキーマで識別子フィールド（identifier field）として指定された列に基づいて、レコードが既に存在する場合は更新し、存在しない場合は新規に挿入します。  \n",
    "アップサート操作は、増分データの取り込みや、マスターデータの更新などのシナリオで非常に便利です。例えば、定期的に更新されるデータソースからのデータを Iceberg テーブルに取り込む場合、アップサート操作を使用することで、既存のレコードを更新し、新しいレコードを追加することができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef01d0df-7c96-4b6a-bcc4-311fe6cc5df7",
   "metadata": {},
   "source": [
    "例えば、都市の人口データを管理するテーブルを考えてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1f4bf1-6978-482f-8002-b5d731bfec72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyiceberg.schema import Schema\n",
    "from pyiceberg.types import IntegerType, NestedField, StringType\n",
    "import pyarrow as pa\n",
    "\n",
    "table_name = \"cities\"\n",
    "table_identifier = f\"{namespace}.{table_name}\"\n",
    "\n",
    "# 都市名を識別子フィールドとして指定したスキーマを定義\n",
    "schema = Schema(\n",
    "    NestedField(1, \"city\", StringType(), required=True),\n",
    "    NestedField(2, \"inhabitants\", IntegerType(), required=True),\n",
    "    # 都市名を識別子フィールドして指定\n",
    "    identifier_field_ids=[1]\n",
    ")\n",
    "\n",
    "if catalog.table_exists(table_identifier):\n",
    "    catalog.purge_table(table_identifier)\n",
    "table = catalog.create_table(table_identifier, schema=schema)\n",
    "\n",
    "arrow_schema = pa.schema([\n",
    "    pa.field(\"city\", pa.string(), nullable=False),\n",
    "    pa.field(\"inhabitants\", pa.int32(), nullable=False),\n",
    "])\n",
    "\n",
    "df = pa.Table.from_pylist([\n",
    "    {\"city\": \"東京\", \"inhabitants\": 13960000},\n",
    "    {\"city\": \"大阪\", \"inhabitants\": 2691000},\n",
    "    {\"city\": \"名古屋\", \"inhabitants\": 2296000},\n",
    "    {\"city\": \"札幌\", \"inhabitants\": 1952000},\n",
    "], schema=arrow_schema)\n",
    "\n",
    "table.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad95ae4e-637c-4109-932f-8dccf2c720b1",
   "metadata": {},
   "source": [
    "次に、いくつかの都市の人口データを更新し、新しい都市のデータを追加するアップサート操作を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd525763-1d78-485a-84a2-d45e7a723df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# アップサート用のデータを準備\n",
    "upsert_data = pa.Table.from_pylist([\n",
    "    # 既存データの更新（名古屋の人口を変更）\n",
    "    {\"city\": \"名古屋\", \"inhabitants\": 2320000},\n",
    "    \n",
    "    # 新規データの挿入（福岡を追加）\n",
    "    {\"city\": \"福岡\", \"inhabitants\": 1588000},\n",
    "    \n",
    "], schema=arrow_schema)\n",
    "\n",
    "# アップサート操作を実行\n",
    "result = table.upsert(upsert_data)\n",
    "\n",
    "# 結果の確認\n",
    "print(f\"更新されたレコード数: {result.rows_updated}\")  # 1（名古屋）\n",
    "print(f\"挿入されたレコード数: {result.rows_inserted}\")  # 1（福岡）\n",
    "table.scan().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1724c458-342e-4653-b77b-b0eeb0c4870d",
   "metadata": {},
   "source": [
    "アップサート操作の結果、名古屋の人口データが更新され、福岡の新しいデータが追加されました。  \n",
    "アップサート操作の戻り値には、更新されたレコード数（`rows_updated`）と挿入されたレコード数（`rows_inserted`）が含まれており、操作の結果を確認できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0634b0f-7bed-4248-a04c-e0716c95a120",
   "metadata": {},
   "source": [
    "### レコードの削除（Delete）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88c08fc-870d-4ba7-9e8f-c719152921d2",
   "metadata": {},
   "source": [
    "`delete` メソッドを使用すると、フィルタ条件に基づいて選択的にレコードを削除できます。  \n",
    "削除操作は、データクレンジング、プライバシー要件への対応、古いデータの整理など、様々なデータ管理タスクで重要な役割を果たします。PyIceberg では、SQL に似た文字列形式または式オブジェクトを使用して削除条件を指定できます。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1e5994-a416-4270-b695-00fe66288e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pyiceberg.schema import Schema\n",
    "from pyiceberg.types import (\n",
    "    TimestampType,\n",
    "    DoubleType,\n",
    "    StringType,\n",
    "    NestedField,\n",
    "    FloatType\n",
    ")\n",
    "\n",
    "table_name = \"observations\"\n",
    "table_identifier = f\"{namespace}.{table_name}\"\n",
    "\n",
    "weather_schema = Schema(\n",
    "    NestedField(field_id=1, name=\"observation_time\", field_type=TimestampType(), required=False),\n",
    "    NestedField(field_id=2, name=\"station_id\", field_type=StringType(), required=False),\n",
    "    NestedField(field_id=3, name=\"temperature\", field_type=DoubleType(), required=False),\n",
    "    NestedField(field_id=4, name=\"humidity\", field_type=FloatType(), required=False),\n",
    "    NestedField(field_id=5, name=\"wind_speed\", field_type=FloatType(), required=False),\n",
    "    NestedField(field_id=6, name=\"precipitation\", field_type=FloatType(), required=False)\n",
    ")\n",
    "\n",
    "if catalog.table_exists(table_identifier):\n",
    "    catalog.purge_table(table_identifier)\n",
    "table = catalog.create_table(\n",
    "    identifier=table_identifier,\n",
    "    schema=weather_schema\n",
    ")\n",
    "\n",
    "\n",
    "data = pa.Table.from_pylist([\n",
    "    {\n",
    "        \"observation_time\": datetime(2025, 4, 28, 12, 0, 0),\n",
    "        \"station_id\": \"TOKYO001\",\n",
    "        \"temperature\": 19.0,\n",
    "        \"humidity\": np.float32(65.2),\n",
    "        \"wind_speed\": np.float32(4.3),\n",
    "        \"precipitation\": np.float32(0.0)\n",
    "    },\n",
    "    {\n",
    "        \"observation_time\": datetime(2020, 4, 28, 13, 0, 0),\n",
    "        \"station_id\": \"OSAKA002\",\n",
    "        \"temperature\": 25.3,\n",
    "        \"humidity\": np.float32(58.6),\n",
    "        \"wind_speed\": np.float32(3.7),\n",
    "        \"precipitation\": np.float32(0.0)\n",
    "    },\n",
    "    {\n",
    "        \"observation_time\": datetime(2025, 4, 28, 14, 0, 0),\n",
    "        \"station_id\": \"NAGOYA003\",\n",
    "        \"temperature\": 22.8,\n",
    "        \"humidity\": np.float32(70.5),\n",
    "        \"wind_speed\": np.float32(5.2),\n",
    "        \"precipitation\": np.float32(1.4)\n",
    "    }\n",
    "])\n",
    "\n",
    "table.append(data)\n",
    "table.scan().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0833a18e-3cdc-4c09-bd0c-2d93030cfe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文字列形式のフィルタを使用した削除\n",
    "# 気温が20度未満のレコードを削除\n",
    "table.delete(delete_filter=\"temperature < 20.0\")\n",
    "\n",
    "# 複合条件を使用した削除\n",
    "# 2022年以前の降水量ゼロのレコードを削除\n",
    "table.delete(delete_filter=\"observation_time < '2023-01-01T00:00:00' AND precipitation = 0.0\")\n",
    "\n",
    "table.scan().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1939f736-43cb-4af0-ae12-c41733e75787",
   "metadata": {},
   "source": [
    "`pyiceberg.expressions` モジュールには、様々な比較演算子や論理演算子が用意されており、これらを組み合わせて複雑なフィルタ条件を構築できます。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eadd5be-7f61-430e-a1b3-fae1beda0f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyiceberg.expressions import LessThan, And, EqualTo, GreaterThanOrEqual\n",
    "from datetime import datetime\n",
    "\n",
    "# 特定の期間内の特定の観測所のデータを削除\n",
    "start_date = datetime(2022, 1, 1)\n",
    "end_date = datetime(2027, 12, 31)\n",
    "\n",
    "# pyiceberg.expressionを使用して削除条件を構築\n",
    "delete_condition = And(\n",
    "    GreaterThanOrEqual(\"observation_time\", start_date),\n",
    "    LessThan(\"observation_time\", end_date),\n",
    "    EqualTo(\"station_id\", \"NAGOYA003\")\n",
    ")\n",
    "\n",
    "table.delete(delete_filter=delete_condition)\n",
    "table.scan().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3fdd63-003c-40d9-8b65-f8152ae36419",
   "metadata": {},
   "source": [
    "削除されたデータは、タイムトラベル機能を使用して過去のスナップショットにアクセスすることで、必要に応じて参照することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b465c23-a086-47fd-92a6-42cf3261fe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 削除直前のスナップショットを取得\n",
    "snapshots = table.inspect.snapshots().to_pandas()\n",
    "pre_delete_snapshot_id = snapshots.iloc[-2][\"snapshot_id\"]\n",
    "\n",
    "# 削除前のデータを参照\n",
    "table.scan(snapshot_id=pre_delete_snapshot_id).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1b7f60-0a75-40cc-890a-683b05b8efb4",
   "metadata": {},
   "source": [
    "### データの追加時の注意点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd11c76-576a-49af-895d-90ce2fe962b4",
   "metadata": {},
   "source": [
    "テーブルに追加するデータは、テーブルのスキーマと一致している必要があります。不一致がある場合はエラーが発生します。\n",
    "特に、PyArrow のスキーマと Iceberg テーブルのスキーマの間で発生しやすい不一致について理解しておくことが重要です。特に注意が必要なのが、タイムスタンプ型と `required` フィールドの扱いです。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ead222c-60e9-4471-91b5-77eb560b4811",
   "metadata": {},
   "source": [
    "Iceberg の `TimestampType` はマイクロ秒精度（'us'）である一方で、PyArrow では秒（'s'）、ミリ秒（'ms'）、マイクロ秒（'us'）、ナノ秒（'ns'）の精度を持つタイムスタンプ型が存在します。PyIceberg では、書き込み時に秒精度とミリ秒精度のタイムスタンプをマイクロ秒精度に変換しますが、ナノ秒精度のタイムスタンプについては設定によってマイクロ秒精度にダウンキャストする必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5722e972-aeb2-47e3-954f-550a82ddc3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "from datetime import datetime\n",
    "\n",
    "# タイムスタンプ精度テスト\n",
    "print(\"タイムスタンプ精度テスト:\")\n",
    "for precision in [\"s\", \"ms\", \"us\", \"ns\"]:\n",
    "    try:\n",
    "        data = pa.Table.from_pylist([\n",
    "            {\n",
    "                \"observation_time\": datetime(2025, 4, 28, 12, 0, 0),\n",
    "                \"station_id\": f\"STATION_{precision}\",\n",
    "                \"temperature\": 23.5, \n",
    "                \"humidity\": np.float32(65.0),\n",
    "                \"wind_speed\": np.float32(3.2),\n",
    "                \"precipitation\": np.float32(0.0)\n",
    "            }\n",
    "        ], schema=pa.schema([\n",
    "            pa.field(\"observation_time\", pa.timestamp(precision)),\n",
    "            pa.field(\"station_id\", pa.string()),\n",
    "            pa.field(\"temperature\", pa.float64()),\n",
    "            pa.field(\"humidity\", pa.float32()),\n",
    "            pa.field(\"wind_speed\", pa.float32()),\n",
    "            pa.field(\"precipitation\", pa.float32())\n",
    "        ]))\n",
    "        table.append(data)\n",
    "        print(f\"{precision}: 成功\")\n",
    "    except Exception as e:\n",
    "        print(f\"{precision}: 失敗 - {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22e026e-f25a-4ccd-9b58-aac5f334ae12",
   "metadata": {},
   "source": [
    "PyIceberg のスキーマ定義では、各フィールドに required=True または required=False を指定することで、そのフィールドが必須かどうかを定義します。PyArrow のスキーマでも同様に nullable=False または nullable=True を指定できます。Iceberg テーブルで required=True に設定されたフィールドに対して、PyArrow 側で nullable=True に設定されたカラムを含むテーブルを作成した場合、実際のレコードが null 値を含むかどうかに関わらず、データ追加時にエラーが発生します。  \n",
    "PyArrow のスキーマでは、デフォルトで nullable=True となるため、Iceberg テーブルのスキーマと一致させるためには、PyArrow のスキーマを定義する際に明示的に nullable=False を指定する必要があります。  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b08f215-c951-4618-bb76-60c6e201339d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプルテーブル作成\n",
    "from pyiceberg.schema import Schema\n",
    "from pyiceberg.types import (\n",
    "    TimestampType,\n",
    "    DoubleType,\n",
    "    StringType,\n",
    "    NestedField,\n",
    "    FloatType\n",
    ")\n",
    "\n",
    "table_name = \"observations\"\n",
    "table_identifier = f\"{namespace}.{table_name}\"\n",
    "\n",
    "weather_schema = Schema(\n",
    "    NestedField(field_id=1, name=\"observation_time\", field_type=TimestampType(), required=False),\n",
    "    NestedField(field_id=2, name=\"station_id\", field_type=StringType(), required=True), # required=True を設定\n",
    "    NestedField(field_id=3, name=\"temperature\", field_type=DoubleType(), required=False),\n",
    "    NestedField(field_id=4, name=\"humidity\", field_type=FloatType(), required=False),\n",
    "    NestedField(field_id=5, name=\"wind_speed\", field_type=FloatType(), required=False),\n",
    "    NestedField(field_id=6, name=\"precipitation\", field_type=FloatType(), required=False)\n",
    ")\n",
    "\n",
    "\n",
    "if catalog.table_exists(table_identifier):\n",
    "    catalog.purge_table(table_identifier)\n",
    "\n",
    "table = catalog.create_table(\n",
    "    identifier=table_identifier,\n",
    "    schema=weather_schema\n",
    ")\n",
    "\n",
    "data = pa.Table.from_pylist([\n",
    "    {\n",
    "        \"observation_time\": datetime(2025, 4, 28, 12, 0, 0),\n",
    "        \"station_id\": \"STATION_001\",  # 実際の値はnullではないが、スキーマ定義の不一致でエラーになる\n",
    "        \"temperature\": 23.5\n",
    "    }\n",
    "], schema=pa.schema([\n",
    "    pa.field(\"observation_time\", pa.timestamp(\"us\")),  \n",
    "    pa.field(\"station_id\", pa.string()),  # PyArrowでは、デフォルトで nullable=Trueとなる\n",
    "    pa.field(\"temperature\", pa.float64())\n",
    "]))\n",
    "\n",
    "table.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3156f7-f30f-43c0-af4e-d30d4a0e19d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pa.Table.from_pylist([\n",
    "    {\n",
    "        \"observation_time\": datetime(2025, 4, 28, 12, 0, 0),\n",
    "        \"station_id\": \"STATION_001\",\n",
    "        \"temperature\": 23.5\n",
    "    }\n",
    "], schema=pa.schema([\n",
    "    pa.field(\"observation_time\", pa.timestamp(\"us\")),  \n",
    "    pa.field(\"station_id\", pa.string(), nullable=False), # 明示的にnullable=Falseを指定する必要がある\n",
    "    pa.field(\"temperature\", pa.float64())\n",
    "]))\n",
    "\n",
    "table.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5a12bd",
   "metadata": {},
   "source": [
    "## テーブルの参照"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca6b287-5870-4d48-aa91-e04420f3e8f2",
   "metadata": {},
   "source": [
    "`scan` メソッドを使用することで、テーブル全体を対象にデータを取得できます。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b31fb46-ec2f-4f21-9b61-90ae4cce768d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pyiceberg.schema import Schema\n",
    "from pyiceberg.types import (\n",
    "    TimestampType,\n",
    "    DoubleType,\n",
    "    StringType,\n",
    "    NestedField,\n",
    "    FloatType\n",
    ")\n",
    "\n",
    "table_name = \"observations\"\n",
    "table_identifier = f\"{namespace}.{table_name}\"\n",
    "\n",
    "weather_schema = Schema(\n",
    "    NestedField(field_id=1, name=\"observation_time\", field_type=TimestampType(), required=False),\n",
    "    NestedField(field_id=2, name=\"station_id\", field_type=StringType(), required=False),\n",
    "    NestedField(field_id=3, name=\"temperature\", field_type=DoubleType(), required=False),\n",
    "    NestedField(field_id=4, name=\"humidity\", field_type=FloatType(), required=False),\n",
    "    NestedField(field_id=5, name=\"wind_speed\", field_type=FloatType(), required=False),\n",
    "    NestedField(field_id=6, name=\"precipitation\", field_type=FloatType(), required=False)\n",
    ")\n",
    "\n",
    "if catalog.table_exists(table_identifier):\n",
    "    catalog.purge_table(table_identifier)\n",
    "table = catalog.create_table(\n",
    "    identifier=table_identifier,\n",
    "    schema=weather_schema\n",
    ")\n",
    "\n",
    "\n",
    "data = pa.Table.from_pylist([\n",
    "    {\n",
    "        \"observation_time\": datetime(2025, 4, 28, 12, 0, 0),\n",
    "        \"station_id\": \"TOKYO001\",\n",
    "        \"temperature\": 19.0,\n",
    "        \"humidity\": np.float32(65.2),\n",
    "        \"wind_speed\": np.float32(4.3),\n",
    "        \"precipitation\": np.float32(0.0)\n",
    "    },\n",
    "    {\n",
    "        \"observation_time\": datetime(2020, 4, 28, 13, 0, 0),\n",
    "        \"station_id\": \"OSAKA002\",\n",
    "        \"temperature\": 25.3,\n",
    "        \"humidity\": np.float32(58.6),\n",
    "        \"wind_speed\": np.float32(3.7),\n",
    "        \"precipitation\": np.float32(0.0)\n",
    "    },\n",
    "    {\n",
    "        \"observation_time\": datetime(2025, 4, 28, 14, 0, 0),\n",
    "        \"station_id\": \"NAGOYA003\",\n",
    "        \"temperature\": 22.8,\n",
    "        \"humidity\": np.float32(70.5),\n",
    "        \"wind_speed\": np.float32(5.2),\n",
    "        \"precipitation\": np.float32(1.4)\n",
    "    }\n",
    "])\n",
    "\n",
    "table.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a9626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# テーブル全体をスキャン\n",
    "scan = table.scan()\n",
    "\n",
    "# スキャン結果をPandasデータフレームとして取得\n",
    "scan.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204be2a7-ef0e-48a6-8ff4-10b7a41cc014",
   "metadata": {},
   "source": [
    "PyIceberg では、テーブルからデータを読み取る際に、`scan` メソッドを使用してフィルタリングや列の選択が可能です。フィルタリングは単なる利便性だけでなく、パフォーマンスと効率性の観点からも重要な意味を持ちます。PyIceberg はフィルタ条件をストレージレイヤーまで「プッシュダウン」することで、必要なデータのみを読み込み、メモリ使用量を削減し、処理速度を向上させます。これにより、大規模なテーブルであっても、軽量な環境で効率的に操作できるようになります。  \n",
    "\n",
    "特に、パーティション列に対するフィルタは「パーティションプルーニング」と呼ばれる最適化が適用され、条件に一致するパーティションのデータファイルのみが読み込まれます。また、データファイルのメタデータに記録された統計情報（列の最小値・最大値など）を活用して、条件に一致しないファイルを読み込み前にスキップする「ファイルスキッピング」も行われます。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69258bc0-7999-4747-8197-ea49257a503f",
   "metadata": {},
   "source": [
    "### 文字列形式のフィルタ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a49eb9-96e1-4986-a7ce-435803489112",
   "metadata": {},
   "source": [
    "文字列形式でフィルタを指定することができます。これは特に動的にフィルタを構築する場合や、簡潔に条件を表現したい場合に便利です"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abc75b4-c9a7-428e-8ffd-63c9a0b2a170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本的な比較演算子\n",
    "table.scan(row_filter=\"temperature >= 25.0\").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4b0800-081e-4b1c-90b3-4be1b909fd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 論理演算子を使った複合条件\n",
    "string_filter2 = table.scan(\n",
    "    row_filter=\"temperature >= 25.0 and humidity > 60.0\"\n",
    ").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebb23f2-7786-4a92-8d8a-0f7621bd5015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# より複雑な条件\n",
    "table.scan(\n",
    "    row_filter=\"(temperature >= 15.0 and humidity > 60.0) or precipitation > 0\"\n",
    ").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102fe882-480a-4248-83e6-a09df72f8723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN演算子の使用\n",
    "table.scan(\n",
    "    row_filter=\"station_id in ('TOKYO001', 'OSAKA002', 'NAGOYA003')\"\n",
    ").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbf205f-e7a6-4e9c-baa8-dcd6b4a6dbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NULL値の処理\n",
    "table.scan(\n",
    "    row_filter=\"humidity is not null and wind_speed is not null\"\n",
    ").to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ce254c-8456-4856-b618-5b7370c22ce2",
   "metadata": {},
   "source": [
    "### 条件式を使用したフィルタリング"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f480a341-2c25-4d8a-85dd-4ed83a8b771b",
   "metadata": {},
   "source": [
    "`pyiceberg.expressions` モジュールが提供する様々な比較演算子や論理演算子によって、複雑なフィルタ条件を構築できます。これにより、特定の条件に一致するデータのみを効率的に取得できます。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a64c6b-2585-4c1f-b337-6912942783d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyiceberg.expressions import (\n",
    "    GreaterThanOrEqual, LessThan, And, Or, NotNull, IsNull, In, NotIn, EqualTo\n",
    ")\n",
    "\n",
    "# 基本的なフィルタ条件\n",
    "table.scan(\n",
    "    row_filter=GreaterThanOrEqual(\"temperature\", 25.0),\n",
    "    selected_fields=(\"observation_time\", \"station_id\", \"temperature\")\n",
    ").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136b6aa5-abb0-417a-9dad-13a6175feb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 複合条件（25度以上かつ東京の観測所のデータ）\n",
    "complex_filter = And(\n",
    "    GreaterThanOrEqual(\"temperature\", 15.0),\n",
    "    EqualTo(\"station_id\", \"TOKYO001\")\n",
    ")\n",
    "table.scan(row_filter=complex_filter).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f55fbc1-27ea-4cc0-9663-20662ea3e80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NULL値の除外（湿度データが欠損していないレコードのみ）\n",
    "not_null_filter = NotNull(\"humidity\")\n",
    "table.scan(row_filter=not_null_filter).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36543dd4-1888-470c-b8e6-2953eb6d82ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 値のリストによるフィルタリング（特定の観測所のデータのみ）\n",
    "stations_filter = In(\"station_id\", [\"TOKYO001\", \"OSAKA002\", \"NAGOYA003\"])\n",
    "table.scan(row_filter=stations_filter).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3194d8-39b1-4c8d-acb3-01500e647a80",
   "metadata": {},
   "source": [
    "### フィルタと列選択の組み合わせ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3333bb-2b74-4242-8428-2e705fb0fe54",
   "metadata": {},
   "source": [
    "フィルタと列選択を組み合わせることで、必要な情報のみを効率的に取得できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbb6989-a139-491e-994d-2e2f3d10e7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一定以上の気温データの観測時間と気温のみを取得\n",
    "table.scan(\n",
    "    row_filter=\"temperature > 15.0\",\n",
    "    selected_fields=[\"observation_time\", \"station_id\", \"temperature\"]\n",
    ").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb4532c-c31c-40d0-a4b0-69de8238c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特定の条件に基づいて異なる分析を行う\n",
    "def analyze_weather_patterns(table, min_temp, max_temp):\n",
    "    # 温度範囲内のデータを取得\n",
    "    temp_range_data = table.scan(\n",
    "        row_filter=f\"temperature >= {min_temp} and temperature <= {max_temp}\",\n",
    "        selected_fields=[\"observation_time\", \"station_id\", \"temperature\", \"humidity\"]\n",
    "    ).to_pandas()\n",
    "    \n",
    "    # 観測所ごとの集計\n",
    "    station_stats = temp_range_data.groupby(\"station_id\").agg({\n",
    "        \"temperature\": [\"mean\", \"min\", \"max\"],\n",
    "        \"humidity\": \"mean\"\n",
    "    })\n",
    "    \n",
    "    return station_stats\n",
    "\n",
    "# 温度範囲ごとの分析結果を比較\n",
    "analyze_weather_patterns(table, 15, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4387e27a-5b7c-4e34-83fd-ae92be432cd2",
   "metadata": {},
   "source": [
    "### 結果の制限"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffec983b-ffdd-48f4-b8c0-25968858f723",
   "metadata": {},
   "source": [
    "スキャン結果の件数を制限する場合は、`limit` パラメータを使用します。これは大規模なテーブルを扱う際に、メモリ使用量を抑えたり、データの探索や検証を効率的に行うために役立ちます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa91db3d-9cbd-4752-bea3-a64686ae5144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最大2件に制限したスキャン\n",
    "limited_scan = table.scan(\n",
    "    selected_fields=(\"observation_time\", \"station_id\", \"temperature\", \"humidity\", \"wind_speed\", \"precipitation\"),\n",
    "    limit=2\n",
    ")\n",
    "\n",
    "limited_scan.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c628085-cb72-408e-b9f9-90253aece59b",
   "metadata": {},
   "source": [
    "## 発展的な活用法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26add66-9607-42b9-893f-ca46c97ef7c6",
   "metadata": {},
   "source": [
    "ここからは、PyIceberg の発展的な機能を活用したデータ管理や分析の方法について説明します。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a0b637-c068-44fb-88b4-88b2d8b8af66",
   "metadata": {},
   "source": [
    "### タイムトラベルクエリとタグの実践的活用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e02aca-07ff-4dfa-89e4-a050f053f953",
   "metadata": {},
   "source": [
    "Iceberg のタイムトラベル機能は、データの履歴を追跡し、過去の任意の時点のデータを参照できる強力な機能です。PyIceberg では、スナップショット ID やタイムスタンプでタイムトラベルを実現できます。また、タグを使用することで、重要なスナップショットを簡単に参照できるようになります。これにより、データの変更履歴を追跡したり、特定の時点のデータを参照したりすることが容易になります。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450df358-023b-4a0c-a753-024ef21cf529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプルテーブル作成\n",
    "from pyiceberg.expressions import EqualTo\n",
    "from pyiceberg.schema import Schema\n",
    "from pyiceberg.types import (\n",
    "    TimestampType,\n",
    "    DoubleType,\n",
    "    StringType,\n",
    "    NestedField,\n",
    "    FloatType\n",
    ")\n",
    "\n",
    "table_name = \"observations\"\n",
    "table_identifier = f\"{namespace}.{table_name}\"\n",
    "\n",
    "weather_schema = Schema(\n",
    "    NestedField(field_id=1, name=\"observation_time\", field_type=TimestampType(), required=False),\n",
    "    NestedField(field_id=2, name=\"station_id\", field_type=StringType(), required=False),\n",
    "    NestedField(field_id=3, name=\"temperature\", field_type=DoubleType(), required=False),\n",
    "    NestedField(field_id=4, name=\"humidity\", field_type=FloatType(), required=False),\n",
    "    NestedField(field_id=5, name=\"wind_speed\", field_type=FloatType(), required=False),\n",
    "    NestedField(field_id=6, name=\"precipitation\", field_type=FloatType(), required=False)\n",
    ")\n",
    "\n",
    "\n",
    "if catalog.table_exists(table_identifier):\n",
    "    catalog.purge_table(table_identifier)\n",
    "\n",
    "table = catalog.create_table(\n",
    "    identifier=table_identifier,\n",
    "    schema=weather_schema\n",
    ")\n",
    "\n",
    "# 初期データ書き込み\n",
    "data = pa.Table.from_pylist([\n",
    "    {\n",
    "        \"observation_time\": datetime(2025, 4, 28, 12, 0, 0),\n",
    "        \"station_id\": \"TOKYO001\",\n",
    "        \"temperature\": 19.0,\n",
    "        \"humidity\": np.float32(65.2),\n",
    "        \"wind_speed\": np.float32(4.3),\n",
    "        \"precipitation\": np.float32(0.0)\n",
    "    },\n",
    "    {\n",
    "        \"observation_time\": datetime(2020, 4, 28, 13, 0, 0),\n",
    "        \"station_id\": \"OSAKA002\",\n",
    "        \"temperature\": 25.3,\n",
    "        \"humidity\": np.float32(58.6),\n",
    "        \"wind_speed\": np.float32(3.7),\n",
    "        \"precipitation\": np.float32(0.0)\n",
    "    },\n",
    "    {\n",
    "        \"observation_time\": datetime(2025, 4, 28, 14, 0, 0),\n",
    "        \"station_id\": \"NAGOYA003\",\n",
    "        \"temperature\": 22.8,\n",
    "        \"humidity\": np.float32(70.5),\n",
    "        \"wind_speed\": np.float32(5.2),\n",
    "        \"precipitation\": np.float32(1.4)\n",
    "    }\n",
    "])\n",
    "\n",
    "table.append(data)\n",
    "# 気温が20度未満のレコードを削除\n",
    "table.delete(delete_filter=\"temperature < 20.0\")\n",
    "table.scan().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b715bc-5f3c-41ea-852d-8ab3d8970830",
   "metadata": {},
   "source": [
    "#### スナップショット ID によるタイムトラベル"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b156e314-82e5-4552-ac2f-446ae8e407bf",
   "metadata": {},
   "source": [
    "まず、テーブルの変更履歴を確認するために、スナップショットの一覧を取得します。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e47c7d2-3986-4065-9513-92119aafd02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# テーブルのスナップショット一覧を取得\n",
    "table.inspect.snapshots().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50954cc1-b079-4c14-befb-a44793500410",
   "metadata": {},
   "source": [
    "特定のスナップショット ID を指定して、その時点のデータを取得できます。以下では、テーブル作成後、初期データが挿入された時点のスナップショット ID を指定して、データを取得しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194bea04-d579-4a6b-b6aa-abaa13cd97bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# スナップショット情報を取得\n",
    "snapshots = table.inspect.snapshots()\n",
    "# committed_atでソートするためのインデックスを取得\n",
    "indices = pa.compute.sort_indices(snapshots[\"committed_at\"])\n",
    "# 最も古いエントリのインデックスを取得\n",
    "oldest_index = indices[0].as_py()\n",
    "# 最も古いsnapshot_idを取得\n",
    "oldest_snapshot_id = snapshots[\"snapshot_id\"][oldest_index].as_py()\n",
    "\n",
    "# スナップショットIDでテーブルをスキャン\n",
    "table.scan(snapshot_id=oldest_snapshot_id).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3b381a-8d8a-4538-948b-17fa797ab126",
   "metadata": {},
   "source": [
    "### タグを使用したスナップショット管理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39f4e48-2214-44c6-8f66-fecf25455130",
   "metadata": {},
   "source": [
    "タグは、特定のスナップショットに任意の名前をつけることで、後から簡単に参照できるようにする仕組みです。これは、ビジネス上の重要なイベントや特定のデータセットを記録する際に便利です。例えば、四半期ごとのレポート作成時のデータスナップショットにタグを付けておくことで、後から簡単にそのデータを参照できます。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50b20ae-36b6-46ed-95cb-c54711b4b6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプルテーブル作成\n",
    "import pyiceberg\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from pyiceberg.catalog import load_catalog\n",
    "from pyiceberg.schema import Schema\n",
    "from pyiceberg.types import (\n",
    "    TimestampType,\n",
    "    DoubleType,\n",
    "    StringType,\n",
    "    NestedField,\n",
    "    FloatType\n",
    ")\n",
    "\n",
    "table_name = \"observations\"\n",
    "table_identifier = f\"{namespace}.{table_name}\"\n",
    "\n",
    "weather_schema = Schema(\n",
    "    NestedField(field_id=1, name=\"observation_time\", field_type=TimestampType(), required=False),\n",
    "    NestedField(field_id=2, name=\"station_id\", field_type=StringType(), required=False),\n",
    "    NestedField(field_id=3, name=\"temperature\", field_type=DoubleType(), required=False),\n",
    "    NestedField(field_id=4, name=\"humidity\", field_type=FloatType(), required=False),\n",
    "    NestedField(field_id=5, name=\"wind_speed\", field_type=FloatType(), required=False),\n",
    "    NestedField(field_id=6, name=\"precipitation\", field_type=FloatType(), required=False)\n",
    ")\n",
    "\n",
    "if catalog.table_exists(table_identifier):\n",
    "    catalog.drop_table(table_identifier)\n",
    "\n",
    "table = catalog.create_table(\n",
    "    identifier=table_identifier,\n",
    "    schema=weather_schema\n",
    ")\n",
    "\n",
    "# 初期データの書き込み - 2025年4月のデータ\n",
    "april_data = pa.Table.from_pylist([\n",
    "    {\n",
    "        \"observation_time\": datetime(2025, 4, 1, 12, 0, 0),\n",
    "        \"station_id\": \"TOKYO001\",\n",
    "        \"temperature\": 19.0,\n",
    "        \"humidity\": np.float32(65.2),\n",
    "        \"wind_speed\": np.float32(4.3),\n",
    "        \"precipitation\": np.float32(0.0)\n",
    "    },\n",
    "    {\n",
    "        \"observation_time\": datetime(2025, 4, 1, 13, 0, 0),\n",
    "        \"station_id\": \"OSAKA002\",\n",
    "        \"temperature\": 25.3,\n",
    "        \"humidity\": np.float32(58.6),\n",
    "        \"wind_speed\": np.float32(3.7),\n",
    "        \"precipitation\": np.float32(0.0)\n",
    "    },\n",
    "    {\n",
    "        \"observation_time\": datetime(2025, 4, 1, 14, 0, 0),\n",
    "        \"station_id\": \"NAGOYA003\",\n",
    "        \"temperature\": 22.8,\n",
    "        \"humidity\": np.float32(70.5),\n",
    "        \"wind_speed\": np.float32(5.2),\n",
    "        \"precipitation\": np.float32(1.4)\n",
    "    }\n",
    "])\n",
    "table.append(april_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f796c94f-d6c2-4fab-8645-3eaa9a8cdc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初期スナップショットに「april_data」というタグをつける\n",
    "# スナップショットIDを取得\n",
    "snapshots = table.snapshots()\n",
    "initial_snapshot_id = table.current_snapshot().snapshot_id\n",
    "\n",
    "# タグを作成\n",
    "with table.manage_snapshots() as snapshots:\n",
    "    snapshots.create_tag(initial_snapshot_id, \"april_data\")\n",
    "    print(f\"初期スナップショット {initial_snapshot_id} に 'april_data' タグを付けました\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5de6f0-94e5-4eb5-84f1-d21c06917bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5月のデータを追加\n",
    "may_data = pa.Table.from_pylist([\n",
    "    {\n",
    "        \"observation_time\": datetime(2025, 5, 1, 12, 0, 0),\n",
    "        \"station_id\": \"TOKYO001\",\n",
    "        \"temperature\": 23.5,\n",
    "        \"humidity\": np.float32(60.0),\n",
    "        \"wind_speed\": np.float32(3.8),\n",
    "        \"precipitation\": np.float32(0.0)\n",
    "    },\n",
    "    {\n",
    "        \"observation_time\": datetime(2025, 5, 1, 13, 0, 0),\n",
    "        \"station_id\": \"OSAKA002\",\n",
    "        \"temperature\": 29.1,\n",
    "        \"humidity\": np.float32(55.3),\n",
    "        \"wind_speed\": np.float32(4.2),\n",
    "        \"precipitation\": np.float32(0.0)\n",
    "    }\n",
    "])\n",
    "table.append(may_data)\n",
    "\n",
    "# 5月のデータを追加した後のスナップショットに「may_data」タグをつける\n",
    "may_snapshot_id = table.current_snapshot().snapshot_id\n",
    "with table.manage_snapshots() as snapshots:\n",
    "    snapshots.create_tag(may_snapshot_id, \"may_data\")\n",
    "    print(f\"5月データ追加後のスナップショット {may_snapshot_id} に 'may_data' タグを付けました\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da26944-1f4d-41f0-8a22-eb4c73861845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 現在のテーブルの状態を確認\n",
    "print(\"\\n現在のテーブルデータ（4月と5月のデータ両方含む）:\")\n",
    "table.scan().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea3fd9c-f9b0-46f2-8c21-f2316b2e93c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# タグ付けされたスナップショットの一覧を表示\n",
    "print(\"\\nテーブルのタグとスナップショット:\")\n",
    "refs = table.metadata.refs\n",
    "for ref_name, ref_details in refs.items():\n",
    "    print(f\"タグ: {ref_name}, スナップショットID: {ref_details.snapshot_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f979a496-5c73-40a3-bc44-6a7eb4f85a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# タグを使用して特定のスナップショットのデータを取得\n",
    "# タグが参照するスナップショットIDを取得\n",
    "april_snapshot_id = None\n",
    "may_snapshot_id = None\n",
    "\n",
    "if \"april_data\" in refs:\n",
    "    april_snapshot_id = refs[\"april_data\"].snapshot_id\n",
    "if \"may_data\" in refs:\n",
    "    may_snapshot_id = refs[\"may_data\"].snapshot_id\n",
    "\n",
    "# 「april_data」タグを使用して4月のデータだけを取得\n",
    "print(\"\\n'april_data'タグを使用して4月のデータを取得:\")\n",
    "table.scan(snapshot_id=april_snapshot_id).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765f83bf-5244-4b9c-9320-b8f96f01be16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 「may_data」タグを使用して5月のデータを含む状態を取得\n",
    "print(\"\\n'may_data'タグを使用して5月までのデータを取得:\")\n",
    "table.scan(snapshot_id=may_snapshot_id).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c553e8-68b3-47eb-a734-56649ad78edc",
   "metadata": {},
   "source": [
    "### 発展的なテーブル定義"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f497df79-0bad-4b5e-a41a-ea59b37c1cdb",
   "metadata": {},
   "source": [
    "PyIcebergでは、テーブルの作成時にパーティショニングやソート順序などの高度な設定を行うことができます。これらの機能を使うことで、テーブルの物理的な構造を制御し、クエリパフォーマンスの最適化に役立てられます。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc05bcf9-31f3-4cbc-8064-a8c2bff23c41",
   "metadata": {},
   "source": [
    "#### パーティションの定義"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea1220f-0807-42e1-b0b3-c22e428bcd3a",
   "metadata": {},
   "source": [
    "パーティショニングは、データを論理的なグループに分割する機能です。PyIcebergでは、様々なパーティション変換を使用して、元のデータ列からパーティション値を生成します。  \n",
    "\n",
    "パーティション仕様を定義するには、`PartitionSpec`クラスと`PartitionField`クラスを使用します：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9e4133-cb67-4c4b-b04f-26f2d26b640a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyiceberg\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from pyiceberg.partitioning import PartitionSpec, PartitionField\n",
    "from pyiceberg.transforms import (\n",
    "    IdentityTransform,\n",
    "    DayTransform,\n",
    ")\n",
    "from pyiceberg.catalog import load_catalog\n",
    "from pyiceberg.schema import Schema\n",
    "from pyiceberg.types import (\n",
    "    TimestampType,\n",
    "    DoubleType,\n",
    "    StringType,\n",
    "    NestedField,\n",
    "    FloatType\n",
    ")\n",
    "\n",
    "table_name = \"observations\"\n",
    "table_identifier = f\"{namespace}.{table_name}\"\n",
    "\n",
    "schema = Schema(\n",
    "    NestedField(field_id=1, name=\"observation_time\", field_type=TimestampType(), required=False),\n",
    "    NestedField(field_id=2, name=\"station_id\", field_type=StringType(), required=False),\n",
    "    NestedField(field_id=3, name=\"temperature\", field_type=DoubleType(), required=False),\n",
    "    NestedField(field_id=4, name=\"humidity\", field_type=FloatType(), required=False),\n",
    "    NestedField(field_id=5, name=\"wind_speed\", field_type=FloatType(), required=False),\n",
    "    NestedField(field_id=6, name=\"precipitation\", field_type=FloatType(), required=False)\n",
    ")\n",
    "\n",
    "# パーティション仕様の定義\n",
    "partition_spec = PartitionSpec(\n",
    "    # observation_timeフィールドを日単位でパーティション化\n",
    "    PartitionField(source_id=1, field_id=1000, transform=DayTransform(), name=\"observation_day\"),\n",
    "    # station_idフィールドをそのままパーティションとして使用\n",
    "    PartitionField(source_id=2, field_id=1001, transform=IdentityTransform(), name=\"station_id\")\n",
    ")\n",
    "\n",
    "if catalog.table_exists(table_identifier):\n",
    "    catalog.drop_table(table_identifier)\n",
    "\n",
    "# パーティション仕様を指定してテーブル作成\n",
    "table = catalog.create_table(\n",
    "    identifier=table_identifier,\n",
    "    schema=schema,\n",
    "    partition_spec=partition_spec\n",
    ")\n",
    "print(\"パーティション定義：\")\n",
    "table.spec()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aa50b2-885d-4f58-86c5-9245dfff05f7",
   "metadata": {},
   "source": [
    "`PartitionField` は、パーティション化するフィールドを定義するためのクラスで、以下のパラメータを指定します：\n",
    "- `source_id`: 元のフィールドのID\n",
    "- `field_id`: パーティションフィールドのID\n",
    "- `transform`: パーティション化に使用する変換（Transform）\n",
    "- `name`: パーティションフィールドの名前\n",
    "\n",
    "`PartitionSpec` は、複数の `PartitionField` を組み合わせてパーティション仕様を定義するためのクラスです。これにより、複数のフィールドを組み合わせた複雑なパーティション化が可能になります。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd50dd12-48c4-422f-b327-f8139c0cb008",
   "metadata": {},
   "source": [
    "PyIcebergでは、以下の Transform がサポートされています：  \n",
    "\n",
    "- `IdentityTransform`: 元の値をそのまま使用します\n",
    "- `BucketTransform`: 値のハッシュに基づいてデータを指定した数のバケットに分割します\n",
    "- `TruncateTransform`: 文字列の先頭から指定した長さの部分を使用します\n",
    "- `YearTransform`: タイムスタンプから年を抽出します\n",
    "- `MonthTransform`: タイムスタンプから月を抽出します\n",
    "- `DayTransform`: タイムスタンプから日を抽出します\n",
    "- `HourTransform`: タイムスタンプから時間を抽出します"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8839839-9ff7-4bfc-bff5-7c1a74270766",
   "metadata": {},
   "source": [
    "### ソート順序の定義"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dd2c1a-789b-4a98-b04a-d72d9160911d",
   "metadata": {},
   "source": [
    "ソート順序は、データファイル内のレコードの配置を制御します。ソート順序を定義するには、`SortOrder`クラスと`SortField`クラスを使用します。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ace25e2-117a-4082-a735-a9bf3a0a5da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyiceberg.table.sorting import SortOrder, SortField\n",
    "from pyiceberg.transforms import IdentityTransform\n",
    "import pyiceberg\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from pyiceberg.partitioning import PartitionSpec, PartitionField\n",
    "from pyiceberg.transforms import (\n",
    "    IdentityTransform,\n",
    "    DayTransform,\n",
    ")\n",
    "from pyiceberg.catalog import load_catalog\n",
    "from pyiceberg.schema import Schema\n",
    "from pyiceberg.types import (\n",
    "    TimestampType,\n",
    "    DoubleType,\n",
    "    StringType,\n",
    "    NestedField,\n",
    "    FloatType\n",
    ")\n",
    "\n",
    "table_name = \"observations\"\n",
    "table_identifier = f\"{namespace}.{table_name}\"\n",
    "\n",
    "schema = Schema(\n",
    "    NestedField(field_id=1, name=\"observation_time\", field_type=TimestampType(), required=False),\n",
    "    NestedField(field_id=2, name=\"station_id\", field_type=StringType(), required=False),\n",
    "    NestedField(field_id=3, name=\"temperature\", field_type=DoubleType(), required=False),\n",
    "    NestedField(field_id=4, name=\"humidity\", field_type=FloatType(), required=False),\n",
    "    NestedField(field_id=5, name=\"wind_speed\", field_type=FloatType(), required=False),\n",
    "    NestedField(field_id=6, name=\"precipitation\", field_type=FloatType(), required=False)\n",
    ")\n",
    "\n",
    "# 単一フィールドによるソート順序の定義\n",
    "simple_sort_order = SortOrder(\n",
    "    SortField(source_id=2, transform=IdentityTransform())  # station_idでソート\n",
    ")\n",
    "\n",
    "if catalog.table_exists(table_identifier):\n",
    "    catalog.drop_table(table_identifier)\n",
    "\n",
    "# ソート順序を指定してテーブルを作成\n",
    "table = catalog.create_table(\n",
    "    identifier=table_identifier,\n",
    "    schema=schema,\n",
    "    sort_order=simple_sort_order\n",
    ")\n",
    "print(\"ソート順定義：\")\n",
    "table.metadata.sort_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bc1ba5-89b2-4d02-89eb-462acef53a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyiceberg.table.sorting import SortOrder, SortField\n",
    "from pyiceberg.transforms import IdentityTransform\n",
    "import pyiceberg\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from pyiceberg.partitioning import PartitionSpec, PartitionField\n",
    "from pyiceberg.catalog import load_catalog\n",
    "from pyiceberg.schema import Schema\n",
    "from pyiceberg.types import (\n",
    "    TimestampType,\n",
    "    DoubleType,\n",
    "    StringType,\n",
    "    NestedField,\n",
    "    FloatType\n",
    ")\n",
    "\n",
    "table_name = \"observations\"\n",
    "table_identifier = f\"{namespace}.{table_name}\"\n",
    "\n",
    "schema = Schema(\n",
    "    NestedField(field_id=1, name=\"observation_time\", field_type=TimestampType(), required=False),\n",
    "    NestedField(field_id=2, name=\"station_id\", field_type=StringType(), required=False),\n",
    "    NestedField(field_id=3, name=\"temperature\", field_type=DoubleType(), required=False),\n",
    "    NestedField(field_id=4, name=\"humidity\", field_type=FloatType(), required=False),\n",
    "    NestedField(field_id=5, name=\"wind_speed\", field_type=FloatType(), required=False),\n",
    "    NestedField(field_id=6, name=\"precipitation\", field_type=FloatType(), required=False)\n",
    ")\n",
    "\n",
    "# 複数フィールドによるソート順序の定義\n",
    "multi_field_sort_order = SortOrder(\n",
    "    SortField(source_id=2, transform=IdentityTransform()),  # まずstation_idでソート\n",
    "    SortField(source_id=1, transform=IdentityTransform())   # 次にobservation_timeでソート\n",
    ")\n",
    "\n",
    "if catalog.table_exists(table_identifier):\n",
    "    catalog.drop_table(table_identifier)\n",
    "\n",
    "# ソート順序を指定してテーブルを作成\n",
    "table = catalog.create_table(\n",
    "    identifier=table_identifier,\n",
    "    schema=schema,\n",
    "    sort_order=multi_field_sort_order\n",
    ")\n",
    "print(\"ソート順定義：\")\n",
    "table.metadata.sort_orders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e07a087-0cd5-41ae-af0e-b28bd2af57b5",
   "metadata": {},
   "source": [
    "`SortField`のパラメータは以下の通りです：  \n",
    "\n",
    "- `source_id`: ソートに使用するフィールドのID\n",
    "- `transform`: フィールド値に適用する変換（パーティションと同様の変換が使用可能）\n",
    "- `direction`: ソート方向（デフォルトは昇順）\n",
    "- `null_order`: NULL値の順序（デフォルトはNULL値を最後に配置）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e412ead8-507a-4517-8d0d-16b2f89d2e48",
   "metadata": {},
   "source": [
    "ソート方向を明示的に指定するには、`SortDirection` を使用します：  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a8fc3b-f841-4207-aa6a-af1a5f9b131d",
   "metadata": {},
   "source": [
    "NULL値の順序も制御できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9296cea1-b2ef-499a-a11c-2c33cd7775b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyiceberg.table.sorting import SortOrder, SortField\n",
    "from pyiceberg.transforms import IdentityTransform\n",
    "import pyiceberg\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from pyiceberg.partitioning import PartitionSpec, PartitionField\n",
    "from pyiceberg.catalog import load_catalog\n",
    "from pyiceberg.schema import Schema\n",
    "from pyiceberg.table.sorting import NullOrder\n",
    "from pyiceberg.types import (\n",
    "    TimestampType,\n",
    "    DoubleType,\n",
    "    StringType,\n",
    "    NestedField,\n",
    "    FloatType\n",
    ")\n",
    "\n",
    "table_name = \"observations\"\n",
    "table_identifier = f\"{namespace}.{table_name}\"\n",
    "\n",
    "schema = Schema(\n",
    "    NestedField(field_id=1, name=\"observation_time\", field_type=TimestampType(), required=False),\n",
    "    NestedField(field_id=2, name=\"station_id\", field_type=StringType(), required=False),\n",
    "    NestedField(field_id=3, name=\"temperature\", field_type=DoubleType(), required=False),\n",
    "    NestedField(field_id=4, name=\"humidity\", field_type=FloatType(), required=False),\n",
    "    NestedField(field_id=5, name=\"wind_speed\", field_type=FloatType(), required=False),\n",
    "    NestedField(field_id=6, name=\"precipitation\", field_type=FloatType(), required=False)\n",
    ")\n",
    "\n",
    "# NULL値の順序を指定したソート\n",
    "null_aware_sort = SortOrder(\n",
    "    SortField(\n",
    "        source_id=4,                           # humidity（欠測値があり得る）\n",
    "        transform=IdentityTransform(),\n",
    "        null_order=NullOrder.NULLS_FIRST      # NULL値を先頭に配置\n",
    "    )\n",
    ")\n",
    "\n",
    "if catalog.table_exists(table_identifier):\n",
    "    catalog.drop_table(table_identifier)\n",
    "\n",
    "# ソート順序を指定してテーブルを作成\n",
    "table = catalog.create_table(\n",
    "    identifier=table_identifier,\n",
    "    schema=schema,\n",
    "    sort_order=null_aware_sort\n",
    ")\n",
    "print(\"ソート順定義：\")\n",
    "table.sort_order()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad1d6f9-667b-46df-a0c3-dea25cf2ce59",
   "metadata": {},
   "source": [
    "パーティションと同様に、ソートフィールドにも変換を適用できます： "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e58230-5fe7-4450-930e-dc289f22cfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyiceberg.table.sorting import SortOrder, SortField\n",
    "from pyiceberg.transforms import IdentityTransform\n",
    "import pyiceberg\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from pyiceberg.partitioning import PartitionSpec, PartitionField\n",
    "from pyiceberg.catalog import load_catalog\n",
    "from pyiceberg.schema import Schema\n",
    "from pyiceberg.table.sorting import SortDirection\n",
    "from pyiceberg.types import (\n",
    "    TimestampType,\n",
    "    DoubleType,\n",
    "    StringType,\n",
    "    NestedField,\n",
    "    FloatType\n",
    ")\n",
    "\n",
    "table_name = \"observations\"\n",
    "table_identifier = f\"{namespace}.{table_name}\"\n",
    "\n",
    "schema = Schema(\n",
    "    NestedField(field_id=1, name=\"observation_time\", field_type=TimestampType(), required=False),\n",
    "    NestedField(field_id=2, name=\"station_id\", field_type=StringType(), required=False),\n",
    "    NestedField(field_id=3, name=\"temperature\", field_type=DoubleType(), required=False),\n",
    "    NestedField(field_id=4, name=\"humidity\", field_type=FloatType(), required=False),\n",
    "    NestedField(field_id=5, name=\"wind_speed\", field_type=FloatType(), required=False),\n",
    "    NestedField(field_id=6, name=\"precipitation\", field_type=FloatType(), required=False)\n",
    ")\n",
    "\n",
    "# 変換を適用したソートフィールド\n",
    "transformed_sort = SortOrder(\n",
    "    SortField(\n",
    "        source_id=1,                           # observation_time\n",
    "        transform=DayTransform(),              # 日付部分のみでソート\n",
    "        direction=SortDirection.DESC           # 新しい日付から古い日付の順\n",
    "    )\n",
    ")\n",
    "\n",
    "if catalog.table_exists(table_identifier):\n",
    "    catalog.drop_table(table_identifier)\n",
    "\n",
    "# ソート順序を指定してテーブルを作成\n",
    "table = catalog.create_table(\n",
    "    identifier=table_identifier,\n",
    "    schema=schema,\n",
    "    sort_order=transformed_sort\n",
    ")\n",
    "print(\"ソート順定義：\")\n",
    "table.sort_order()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd6d7b1",
   "metadata": {},
   "source": [
    "## Python のデータ分析ライブラリとの連携"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8eb762-768c-482b-9a2b-f7572d1407e2",
   "metadata": {},
   "source": [
    "### Pandas との連携"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0b7f55-7a0d-43f0-9ee4-b6a6a4595bc1",
   "metadata": {},
   "source": [
    "Pandas は Python で広く使用されているデータ分析ライブラリです。DataFrame という表形式のデータ構造を提供し、データの操作、変換、可視化、統計分析などの機能を備えています。直感的な API と豊富な機能により、データサイエンティストやアナリストの標準ツールとなっています。\n",
    "\n",
    "PyIceberg と Pandas を組み合わせることで、Iceberg の高度なデータ管理機能と Pandas の使いやすいデータ分析機能を両立できます。PyIceberg が行うフィルタリングはストレージレベルで適用されるため、必要なデータのみを効率的に読み込み、メモリ使用量を最適化できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e60eac-5f4d-4eb6-a929-897337689af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプルテーブル作成\n",
    "from pyiceberg.expressions import EqualTo\n",
    "from pyiceberg.schema import Schema\n",
    "from pyiceberg.types import (\n",
    "    TimestampType,\n",
    "    DoubleType,\n",
    "    StringType,\n",
    "    NestedField,\n",
    "    FloatType\n",
    ")\n",
    "\n",
    "table_name = \"observations\"\n",
    "table_identifier = f\"{namespace}.{table_name}\"\n",
    "\n",
    "weather_schema = Schema(\n",
    "    NestedField(field_id=1, name=\"observation_time\", field_type=TimestampType(), required=False),\n",
    "    NestedField(field_id=2, name=\"station_id\", field_type=StringType(), required=False),\n",
    "    NestedField(field_id=3, name=\"temperature\", field_type=DoubleType(), required=False),\n",
    "    NestedField(field_id=4, name=\"humidity\", field_type=FloatType(), required=False),\n",
    "    NestedField(field_id=5, name=\"wind_speed\", field_type=FloatType(), required=False),\n",
    "    NestedField(field_id=6, name=\"precipitation\", field_type=FloatType(), required=False)\n",
    ")\n",
    "\n",
    "\n",
    "if catalog.table_exists(table_identifier):\n",
    "    catalog.purge_table(table_identifier)\n",
    "\n",
    "table = catalog.create_table(\n",
    "    identifier=table_identifier,\n",
    "    schema=weather_schema\n",
    ")\n",
    "\n",
    "data = pa.Table.from_pylist([\n",
    "    {\n",
    "        \"observation_time\": datetime(2025, 4, 28, 12, 0, 0),\n",
    "        \"station_id\": \"TOKYO001\",\n",
    "        \"temperature\": 19.0,\n",
    "        \"humidity\": np.float32(65.2),\n",
    "        \"wind_speed\": np.float32(4.3),\n",
    "        \"precipitation\": np.float32(0.0)\n",
    "    },\n",
    "    {\n",
    "        \"observation_time\": datetime(2020, 4, 28, 13, 0, 0),\n",
    "        \"station_id\": \"OSAKA002\",\n",
    "        \"temperature\": 25.3,\n",
    "        \"humidity\": np.float32(58.6),\n",
    "        \"wind_speed\": np.float32(3.7),\n",
    "        \"precipitation\": np.float32(0.0)\n",
    "    },\n",
    "    {\n",
    "        \"observation_time\": datetime(2025, 4, 28, 14, 0, 0),\n",
    "        \"station_id\": \"NAGOYA003\",\n",
    "        \"temperature\": 22.8,\n",
    "        \"humidity\": np.float32(70.5),\n",
    "        \"wind_speed\": np.float32(5.2),\n",
    "        \"precipitation\": np.float32(1.4)\n",
    "    }\n",
    "])\n",
    "\n",
    "table.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d507bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# テーブルをスキャンしてPandasデータフレームに変換\n",
    "df = table.scan(\n",
    "    row_filter=\"temperature >= 25.0\",\n",
    "    selected_fields=(\"observation_time\", \"station_id\", \"temperature\", \"humidity\", \"precipitation\")\n",
    ").to_pandas()\n",
    "\n",
    "# Pandasの機能を使用したデータ分析\n",
    "avg_temp_by_station = df.groupby(\"station_id\")[\"temperature\"].mean()\n",
    "print(\"観測所ごとの平均気温:\")\n",
    "print(avg_temp_by_station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30884a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日付ごとの平均気温と湿度\n",
    "df[\"date\"] = df[\"observation_time\"].dt.date\n",
    "daily_stats = df.groupby(\"date\").agg({\n",
    "    \"temperature\": [\"mean\", \"min\", \"max\"],\n",
    "    \"humidity\": [\"mean\", \"min\", \"max\"]\n",
    "})\n",
    "\n",
    "print(\"日付ごとの気象統計:\")\n",
    "print(daily_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98db3f8f-cc69-48d8-b928-053b7606ed2b",
   "metadata": {},
   "source": [
    "### Polars と PyIceberg の連携"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a087133-8aca-43e8-bb91-f8b361b56bd4",
   "metadata": {},
   "source": [
    "Polars は、高性能な列指向データフレームライブラリです。Rust で実装されており、遅延評価（Lazy Evaluation）による最適化などを用いた、高速なパフォーマンスが特徴です。  \n",
    "遅延評価とは、実際にデータが必要になるまで計算を実行せず、最適な実行計画を自動的に構築する機能です。これにより、PyIceberg からの大規模データでも効率的な処理が可能になります。フィルタ条件の最適化や不要な列の読み込み回避などが自動的に行われ、メモリ使用量とパフォーマンスが改善されます。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad63904c-3023-4b08-bd27-793e1354735e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyiceberg[polars] -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eae8d83-ccdf-4d55-bd78-f4d50755659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "# テーブルをスキャンしてPolarsデータフレームに変換\n",
    "polars_df = table.scan(\n",
    "    row_filter=\"observation_time >= 1651234500000000\",\n",
    "    selected_fields=(\"observation_time\", \"station_id\", \"temperature\", \"humidity\", \"precipitation\")\n",
    ").to_polars()\n",
    "\n",
    "# Polarsの機能を使用したデータ分析\n",
    "polars_df.group_by(\"station_id\").agg([\n",
    "    pl.col(\"temperature\").mean().alias(\"avg_temperature\"),\n",
    "    pl.col(\"precipitation\").sum().alias(\"total_precipitation\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62bf1c5-13a3-480d-b433-955a0a7cec2e",
   "metadata": {},
   "source": [
    "また、LazyFrame を使用した遅延評価も可能です："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dee7164-7a18-4337-8b54-278c333ebac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# テーブルからPolars LazyFrameを取得\n",
    "lazy_frame = table.to_polars()\n",
    "\n",
    "# 遅延評価によるフィルタリングと演算\n",
    "result = (\n",
    "    lazy_frame\n",
    "    .filter(pl.col(\"temperature\") > 25.0)\n",
    "    .select([\"station_id\", \"temperature\", \"humidity\"])\n",
    "    .group_by(\"station_id\")\n",
    "    .agg([\n",
    "        pl.col(\"temperature\").mean().alias(\"avg_temperature\"),\n",
    "        pl.col(\"humidity\").mean().alias(\"avg_humidity\")\n",
    "    ])\n",
    "    .collect()  # ここで実際の計算が実行される\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20dfa92-8e48-49f1-8604-fb210d0edbaa",
   "metadata": {},
   "source": [
    "### DuckDB と PyIceberg の連携"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee327333-66dc-4d04-b9dd-de335f00448a",
   "metadata": {},
   "source": [
    "DuckDB は、分析ワークロード向けに設計された高性能な組み込み型 SQL データベースエンジンです。メモリ内で高速に動作し、大規模なデータセットも効率的に処理できます。Python 環境との統合が容易で、SQL の表現力を活かした分析が可能です。\n",
    "\n",
    "PyIceberg と DuckDB の連携により、Iceberg テーブルに対して SQL クエリを実行できます。`to_duckdb()` メソッドを使用して、テーブルを DuckDB に変換するだけで SQL 分析が可能になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367a69cb-b3c0-4b32-9f39-ba7c7159b527",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyiceberg[duckdb] -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa773590-419f-4e9c-9372-36edb999fc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# テーブルをDuckDBに変換\n",
    "con = table.scan().to_duckdb(table_name=\"weather_observations\")\n",
    "\n",
    "# SQLクエリの実行\n",
    "result = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        station_id, \n",
    "        AVG(temperature) as avg_temperature, \n",
    "        SUM(precipitation) as total_precipitation \n",
    "    FROM weather_observations \n",
    "    WHERE temperature > 25.0 \n",
    "    GROUP BY station_id\n",
    "    ORDER BY total_precipitation DESC\n",
    "\"\"\").fetchdf()  # pandas DataFrameとして結果を取得\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9b05a0-561f-43ec-a9ae-9ff54fe2b618",
   "metadata": {},
   "source": [
    "PyIceberg のフィルタ条件と DuckDB の SQL を組み合わせることで、効率的なデータ処理が可能です。例えば、パーティション列に対するフィルタを PyIceberg で適用し、詳細な分析を DuckDB で行うことができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac98f8b-57fa-4f30-a10c-b98b840f7efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyIcebergでフィルタリングしてからDuckDBに変換\n",
    "filtered_scan = table.scan(\n",
    "    row_filter=\"observation_time >= '2023-01-01T00:00:00' and observation_time < '2026-02-01T00:00:00'\",\n",
    "    selected_fields=[\"observation_time\", \"station_id\", \"temperature\", \"humidity\"]\n",
    ")\n",
    "\n",
    "# DuckDBで集計分析\n",
    "con = filtered_scan.to_duckdb(table_name=\"january_observations\")\n",
    "monthly_stats = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        station_id,\n",
    "        AVG(temperature) as avg_temp,\n",
    "        MIN(temperature) as min_temp,\n",
    "        MAX(temperature) as max_temp,\n",
    "        AVG(humidity) as avg_humidity\n",
    "    FROM january_observations\n",
    "    GROUP BY station_id\n",
    "\"\"\").fetchdf()\n",
    "monthly_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0bd335-7d39-47fa-a273-0da10f71f789",
   "metadata": {},
   "source": [
    "## PyIceberg の設定を管理する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3addc2a7-ecbc-4f76-9113-6f2e477ed7f5",
   "metadata": {},
   "source": [
    "PyIceberg では、`.pyiceberg.yaml` という設定ファイルを使用することで、カタログへの接続設定を管理できます。この設定ファイルは PyIceberg のさまざまな動作を制御するために使用され、ユーザーが定義した設定はライブラリの初期化時に読み込まれます。これにより、PyIceberg を使用する際の設定を一元管理でき、複数のプロジェクトや環境での再利用が容易になります。  \n",
    "`.pyiceberg.yaml` ファイルは YAML 形式で記述され、カタログの設定やデフォルトのテーブルプロパティなどを指定できます。設定ファイルは `PYICEBERG_HOME` 環境変数で指定されたディレクトリ、ホームディレクトリ、またはワーキングディレクトリのいずれかに配置できます。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48859be-7b50-408f-a244-e3edd51067cf",
   "metadata": {},
   "source": [
    "本ハンズオン環境には、以下の `.pyiceberg.yaml` が配置されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0430ea0-143c-4903-aacf-d72f72e95001",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat /home/jovyan/.pyiceberg.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee450cb0-f253-4ae2-8514-009f15c8e7c6",
   "metadata": {},
   "source": [
    "この設定ファイルでは、`catalog` セクションでカタログの設定を定義しています。`default` はカタログの名前で、`type` はカタログの種類を指定します。ここでは REST カタログを使用しており、`uri` にはハンズオン環境の REST カタログサーバーの URL を指定しています。また、ストレージ(minio)に関する設定も含まれています。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e03dec5-d064-4052-8957-eb049c7a9578",
   "metadata": {},
   "source": [
    "この設定ファイルによって、ハンズオン環境では、カタログへの接続を定義する際に、`default` を指定するだけで接続できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91eec22-980d-49bc-b6cd-3e321d14e6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iceberg カタログへの接続設定\n",
    "from pyiceberg.catalog import load_catalog\n",
    "catalog = load_catalog(\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38274a7b-242e-4393-a618-b6ee6421c69f",
   "metadata": {},
   "source": [
    "Icebergテーブルの動作に関するプロパティも `.pyiceberg.yaml` で設定できます。例えば、以下のような設定が可能です。  \n",
    "\n",
    "- write.format.default: データファイルのデフォルトフォーマット。parquet, avro, orc から選択。デフォルトは parquet\n",
    "- write.parquet.compression-codec: Parquetの圧縮アルゴリズム。uncompressed, zstd, gzip, snappyから選択。デフォルトは zstd\n",
    "- write.object-storage.enabled: ストレージへの書き込み時に、プレフィックスの分散による最適化をするかどうか。デフォルトは False\n",
    "\n",
    "これらの設定は、PyIceberg でのテーブル操作時のデフォルト値として使用されます\n",
    "\n",
    "設定可能なプロパティの詳細は、PyIceberg の公式ドキュメントを参照してください。  \n",
    "https://py.iceberg.apache.org/configuration/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d267bab-d60f-4b91-959c-d9699083881c",
   "metadata": {},
   "source": [
    "設定ファイルを使用する代わりに、環境変数を通じて設定を提供することも可能です。これらの環境変数は`PYICEBERG_`プレフィックスから始まり、続いてYAML構造に対応する名前を使用します。階層構造は二重アンダースコア`__`で表現され、ダッシュ`-`は単一アンダースコア`_`として表されます。例えば、`PYICEBERG_CATALOG__DEFAULT__S3__ACCESS_KEY_ID` は、YAML ファイルの `catalog.default.s3.access-key-id` に相当します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d35d51-c9b9-40c6-9723-49601f8e52eb",
   "metadata": {},
   "source": [
    "## Python CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360d2ed9-e6ef-4e89-a081-690387e85e19",
   "metadata": {},
   "source": [
    "PyIceberg には、Iceberg テーブルのメタデータを参照・管理するためのコマンドラインインターフェース（CLI）が含まれています。この CLI は主にテーブルの一覧表示、スキーマの確認、テーブルプロパティの管理などのメタデータ操作に特化しており、テーブルデータの読み書きは行いません。Python API を使用せずとも、コマンドラインから直接 Iceberg テーブルのメタデータを操作できるため、テーブル構造の調査や設定の確認・変更に役立ちます。また、運用系のスクリプトやデータパイプラインの一部に組み込んで活用できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796d70eb-bffe-43dd-a14d-a875669e26b8",
   "metadata": {},
   "source": [
    "### 基本的な使い方\n",
    "PyIceberg CLI の基本的な構文は以下の通りです：  \n",
    "\n",
    "```bash\n",
    "pyiceberg [オプション] コマンド [引数]\n",
    "```\n",
    "\n",
    "主なオプションには以下があります：\n",
    "\n",
    "- `--catalog`: 使用するカタログを指定します。設定ファイルに複数のカタログが定義されている場合に、どのカタログを使用するかを明示的に指定できます。\n",
    "- `--output`: 出力形式を指定します。`text`（人間が読みやすい形式）または`json`（構造化されたデータ形式）が選択できます。特にスクリプトでの自動処理を行う場合は`json`形式が便利です。\n",
    "- `--uri`: カタログのURIを直接指定します。設定ファイルを使用せずに一時的にカタログに接続する場合に便利です。\n",
    "- `--credential`: 認証情報を直接指定します。セキュリティ上の理由から、一時的な接続や特定の操作にのみ使用することをお勧めします。\n",
    "- `--verbose`: 詳細な出力を有効化します。トラブルシューティングや詳細なログが必要な場合に役立ちます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d40e6f8-9fe0-4664-962e-89a7b1e8d4b4",
   "metadata": {},
   "source": [
    "オプションを明示的に指定しない場合、PyIceberg CLI は `.pyiceberg.yaml` で定義された `default` として定義されたカタログを使用します。これにより、CLI を使用する際に毎回オプションを指定する手間が省けます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f9a6fe-9956-4e78-bcc2-28dc72adaf37",
   "metadata": {},
   "source": [
    "### 主要なコマンド"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a209e79d-81a7-4df9-80dc-5a4dc928a334",
   "metadata": {},
   "source": [
    "PyIceberg CLI には、テーブルメタデータを管理するための様々なコマンドが用意されています。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f94a42d-056c-400d-952e-1de51be8122f",
   "metadata": {},
   "source": [
    "#### 名前空間とテーブルの探索"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c82a200-0b81-4dce-bd9b-44601da39113",
   "metadata": {},
   "source": [
    "`list` コマンドは、利用可能な名前空間やテーブルを一覧表示します。引数なしで実行すると名前空間の一覧を、名前空間を指定すると、その名前空間内のテーブル一覧を表示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc07279-aa1b-4f42-acdf-8c73e567026a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 名前空間の一覧を表示\n",
    "!pyiceberg list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03636cb0-de75-479e-9cc2-1557d62c4939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特定の名前空間内のテーブル一覧を表示\n",
    "!pyiceberg list pyIceberg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a7f733-5568-41d2-bfba-c9b444cce208",
   "metadata": {},
   "source": [
    "`list` コマンドを使えば、どのようなテーブルが利用可能かを簡単に確認できます。また、スクリプトで特定の名前空間内のすべてのテーブルに対して一括処理を行う際にも、このコマンドでテーブル一覧を取得できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e0be7e-583a-49ce-a91b-2dfa795515fb",
   "metadata": {},
   "source": [
    "#### テーブルメタデータの詳細表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a3010c-09db-40eb-9585-734334300632",
   "metadata": {},
   "source": [
    "`describe` コマンドは、テーブルの詳細情報を表示します。テーブル形式バージョン、メタデータの場所、UUID、最終更新日時、パーティション仕様、ソート順序、現在のスキーマなど、テーブルの構造と設定に関する包括的な情報が得られます。また、テーブルの最終更新日時を確認してデータの鮮度を把握したり、メタデータファイルの場所を確認して低レベルの調査や問題解決を行ったりする際にも役立ちます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d77810b-a6ce-4d0c-8b26-c3612f9ded0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pyiceberg describe pyIceberg.observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80afbbf-7689-4a8d-ac2e-c7d6e880bbf1",
   "metadata": {},
   "source": [
    "#### テーブルプロパティの管理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08df944b-a7a5-4a51-8637-c0de747c81e8",
   "metadata": {},
   "source": [
    "`properties` コマンドは、テーブルのプロパティを表示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8705a911-d436-4862-88d4-1214857d92db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pyiceberg properties get table pyIceberg.observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3fb032-84e1-463d-a5bd-2fd89a68c3a8",
   "metadata": {},
   "source": [
    "#### テーブルのロケーション情報の表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa061dfc-fbd5-471d-9337-588a85a4c44f",
   "metadata": {},
   "source": [
    "`location` コマンドは、テーブルのベースロケーション（データファイルが格納されている場所）を表示します。これにより、テーブルの物理的な保存場所を確認できます。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b942372-67bf-472b-9a44-2107e84aa88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pyiceberg location pyIceberg.observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120894d7-57be-4320-9c95-21d8c931fc4c",
   "metadata": {},
   "source": [
    "### テーブルのUUID取得"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e369809d-4211-4b8e-bd4e-00a5752df0b3",
   "metadata": {},
   "source": [
    "`uuid` コマンドは、テーブルの一意識別子（UUID）を表示します。この UUID はテーブルを一意に識別するために使用され、テーブル名が変更されても変わりません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e534f3ad-dfb4-436d-aefc-7129d9ab9786",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pyiceberg uuid pyIceberg.observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8517475d",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "本章では、Python から直接 Iceberg テーブルを操作できる PyIceberg について解説しました。PyIceberg は分散処理環境や Java の実行環境を必要とせず、軽量な環境で Iceberg の高度な機能を活用できるクライアントライブラリです。  \n",
    "PyIceberg の主な特徴として、ACIDトランザクション、スキーマの進化、タイムトラベルなどの Iceberg の核となる機能を Python 環境から直接利用できる点、そして Pandas、Polars などの主要な Python データ分析ライブラリと統合されている点が挙げられます。これにより、データサイエンティストやアナリスト、アプリケーション開発者が、既存の Python ベースのワークフローを活かしながら Iceberg テーブルを効率的に操作できます。  \n",
    "PyIceberg は特に、ローカル開発環境でのデータ分析、データサイエンスにおける特徴量エンジニアリング、イベント駆動型データ処理など、様々なユースケースで効果を発揮します。同時に、PyIceberg で管理するテーブルは Spark、Trino などの分散処理エンジンからも操作できるため、データの成長に合わせた柔軟なスケーリングが可能です。  \n",
    "\n",
    "PyIceberg は軽量なセットアップと低い学習コストで、小規模から中規模のデータを対象とした環境において Iceberg の恩恵を受けるための理想的な入口となり、Python エコシステムとの統合によってより多くのユーザーが Iceberg を活用する可能性を開いています。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928b4dbb-841f-491d-bbc8-7d57ad3054ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
